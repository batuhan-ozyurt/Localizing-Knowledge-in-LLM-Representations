{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f4e7bca-933a-4aed-a160-8c3298b83cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "import gc\n",
    "\n",
    "import sys\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/utilities')\n",
    "sys.path.append(\"/scratch/users/bozyurt20/hpc_run/blobs/\")\n",
    "from util_research import *\n",
    "\n",
    "max_len = 512\n",
    "num_layers = 24\n",
    "d_model = 4096\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\", truncation_side=\"right\", add_prefix_space=True)\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f5e2c18-eafa-4287-9a70-ed7dfce62856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overriding torch_dtype=None with `torch_dtype=torch.float16` due to requirements of `bitsandbytes` to enable model loading in mixed int8. Either pass torch_dtype=torch.float16 or don't pass this argument at all to remove this warning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "CUDA SETUP: CUDA runtime path found: /kuacc/users/bozyurt20/.conda/envs/hf/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 7.0\n",
      "CUDA SETUP: Detected CUDA version 110\n",
      "CUDA SETUP: Loading binary /kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/bitsandbytes/libbitsandbytes_cuda110_nocublaslt.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!\n",
      "  warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\", device_map=\"balanced\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9834d95-1d5a-4df0-a222-a25e6999bbee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_str = \"I want you to repeat the word hello ten times. \"\n",
    "len(tokenizer.encode(my_str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb9ebad7-2874-4c58-8f2e-e3f159fdafc1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85.33333333333333"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1024 / 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae84d08e-a275-4395-beef-f4e98f44fef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "long_str = my_str * 130\n",
    "input_ids = tokenizer.encode(long_str, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3b8c89d-ec43-4f3e-b0d1-ed7387e949c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1562"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8fe90e4f-029e-4c2d-8736-b6564a61efd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad> Yes</s>'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.generate(input_ids, max_length=20)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34b77f46-fb0f-4f49-8879-d3aadc379a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del input_ids\n",
    "del out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8e2b6f5b-10cd-4d0b-bf3b-7e197d3f1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db9cf916-8b38-49ae-961c-5b00c5fc4b3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<pad> Yes</s>'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    out = model.generate(input_ids, max_length=20)\n",
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa88ca00-a859-4eae-b501-e6cafc7e8f89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a865b8-8259-4d42-9b4b-adb36aed86db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
