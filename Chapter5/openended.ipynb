{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b771d45-8e5a-474d-963e-b689a537bb08",
   "metadata": {},
   "source": [
    "# Necessary Imports and Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a29b66c-06ac-4b12-b842-5b6fe4cbae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import transformers\n",
    "import torch\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "import xmltodict\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from fuzzywuzzy import fuzz\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import sys\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/utilities')\n",
    "sys.path.append(\"/scratch/users/bozyurt20/hpc_run/blobs/\")\n",
    "from util_research import *\n",
    "\n",
    "#from toy_dataset import contexts\n",
    "\n",
    "max_len = 512\n",
    "num_layers = 24\n",
    "d_model = 4096"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3616a28-17ae-4e77-9a93-c55aa51146b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bigscience/T0pp\", truncation_side=\"right\", add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a295e2f-bfdd-4f76-87d4-f92ce640df21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"bigscience/T0pp\")#, device_map=\"auto\", load_in_8bit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ee00389-021d-4dd0-81ae-e37166c7b2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_one(input_ids, entity_str, index):\n",
    "    \n",
    "    entity_id = tokenizer.encode(entity_str)\n",
    "    \n",
    "    if len(entity_id) != 2:\n",
    "        print(\"Not an appropriate entity!\")\n",
    "        return\n",
    "    \n",
    "    entity_id = entity_id[0]\n",
    "    \n",
    "    input_ids_list = input_ids.tolist()\n",
    "\n",
    "    all_entity_mention_indices = []\n",
    "    for i, j in enumerate(input_ids_list[0]):\n",
    "        if j == entity_id:\n",
    "            all_entity_mention_indices.append(i)\n",
    "    try:\n",
    "        entity_ind = all_entity_mention_indices[index]\n",
    "        return entity_ind\n",
    "    except:\n",
    "        print(\"entity not found in the input!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9d3a60-7121-41db-9d96-9e5351e43bdc",
   "metadata": {},
   "source": [
    "# Open-Ended Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420104f-cb33-4e01-8cdc-9259c78161d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_one(input_ids, entity_str, index):\n",
    "    \n",
    "    entity_id = tokenizer.encode(entity_str)\n",
    "    \n",
    "    if len(entity_id) != 2:\n",
    "        print(\"Not an appropriate entity!\")\n",
    "        return\n",
    "    \n",
    "    entity_id = entity_id[0]\n",
    "    \n",
    "    input_ids_list = input_ids.tolist()\n",
    "\n",
    "    all_entity_mention_indices = []\n",
    "    for i, j in enumerate(input_ids_list[0]):\n",
    "        if j == entity_id:\n",
    "            all_entity_mention_indices.append(i)\n",
    "    try:\n",
    "        entity_ind = all_entity_mention_indices[index]\n",
    "        return entity_ind\n",
    "    except:\n",
    "        print(\"entity not found in the input!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1656e5-c150-4b9f-a21a-3a936da0eaf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 1) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e10217-b595-4fd1-91aa-c82a134c1aa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = tm8\n",
    "improvements = []\n",
    "for context_previous in sentences:\n",
    "    \n",
    "    char = context_previous.split()[0]\n",
    "    answer = context_previous.split()[-1][:-1]\n",
    "    question = \"Where was \" + char + \"?\"\n",
    "    entity_hidden_state = previous_timestep(context_previous, char)\n",
    "    print(char)\n",
    "    print(answer)\n",
    "    print(question)\n",
    "    \n",
    "    context_current = \"Lucas was 30 years old.\"\n",
    "    probability, scores = current_timestep_regular(context_current, question, answer, template)\n",
    "    \n",
    "    context_enhanced = char_previous + \" \" + context_current\n",
    "    print(context_enhanced)\n",
    "    probability_enhanced, scores_enhanced = current_timestep_enhanced(context_enhanced, \n",
    "                                                                      question, \n",
    "                                                                      answer, \n",
    "                                                                      template, \n",
    "                                                                      char, \n",
    "                                                                      entity_hidden_state)\n",
    "    \n",
    "    improvement = probability_enhanced-probability\n",
    "    print(improvement)\n",
    "    improvements.append( improvement )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5e991e-3d7b-4562-945d-a489aec9fdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(improvements)/len(improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec7a2f-51ca-4062-ac7a-d36894a9c8df",
   "metadata": {},
   "source": [
    "# Open-Ended Generation - Where did he travel to? - Character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d35e9cf1-bd3c-42ce-a442-369160e860d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question)\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question)\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 1) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4211b3ad-d896-4bc4-8124-060d38909686",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "London\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.013598007551081537\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Paris\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.030700638656128376\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Oslo\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-4.614257318813486e-05\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Istanbul\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0016489084038636692\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Beijing\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.00016208096363357072\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Sydney\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0006657935546172666\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Cairo\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.00013925467712994077\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Seoul\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-8.202939743418014e-05\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Rome\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05106335350728841\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "John\n",
      "Prague\n",
      "Where did John travel to?\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Question: \"Where did John travel to?\"\n",
      "Context: \"John Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0005500188563019037\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "Harry\n",
      "London\n",
      "Where did Harry travel to?\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "Harry Lucas was 30 years old.\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Harry Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.030317068398289848\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "Harry\n",
      "Paris\n",
      "Where did Harry travel to?\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "Harry Lucas was 30 years old.\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Harry Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.051484577349356186\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "Harry\n",
      "Oslo\n",
      "Where did Harry travel to?\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "Harry Lucas was 30 years old.\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Harry Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-1.7130933818165772e-05\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "Harry\n",
      "Istanbul\n",
      "Where did Harry travel to?\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "Harry Lucas was 30 years old.\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Harry Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0013133715082571662\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "Harry\n",
      "Beijing\n",
      "Where did Harry travel to?\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "Harry Lucas was 30 years old.\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Harry Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-6.846164848184344e-05\n",
      "new code working-modeling_t5\n",
      "entity_ind: 0\n",
      "Harry\n",
      "Sydney\n",
      "Where did Harry travel to?\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Lucas was 30 years old.\"\n",
      "Answer:\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "Harry Lucas was 30 years old.\n",
      "Question: \"Where did Harry travel to?\"\n",
      "Context: \"Harry Lucas was 30 years old.\"\n",
      "Answer:\n",
      "entity not found in the input!\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n"
     ]
    }
   ],
   "source": [
    "template = tm8\n",
    "improvements = []\n",
    "for context_previous in sentences:\n",
    "    \n",
    "    char = context_previous.split()[0]\n",
    "    answer = context_previous.split()[-1][:-1]\n",
    "    question = \"Where did \" + char + \" travel to?\"\n",
    "    entity_hidden_state = previous_timestep(context_previous, char)\n",
    "    print(char)\n",
    "    print(answer)\n",
    "    print(question)\n",
    "    \n",
    "    context_current = \"Lucas was 30 years old.\"\n",
    "    probability, scores = current_timestep_regular(context_current, question, answer, template)\n",
    "    \n",
    "    context_enhanced = char + \" \" + context_current\n",
    "    print(context_enhanced)\n",
    "    probability_enhanced, scores_enhanced = current_timestep_enhanced(context_enhanced, \n",
    "                                                                      question, \n",
    "                                                                      answer, \n",
    "                                                                      template, \n",
    "                                                                      char, \n",
    "                                                                      entity_hidden_state)\n",
    "    \n",
    "    improvement = probability_enhanced-probability\n",
    "    print(improvement)\n",
    "    improvements.append( improvement )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f842970b-7bdf-4ceb-9588-dfd27ab7fe04",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0317173b-e1e0-4e67-b0c6-3bc83dbeab6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"chars\"] = improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "88483bac-af52-4dac-983a-814e4d73dac9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.305687400116832e-05"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(improvements)/len(improvements)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3266cb-bd43-4f25-993b-553be237c990",
   "metadata": {},
   "source": [
    "# Open-Ended Generation - Where did he travel to? - \"travelled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f72694-77de-4c82-9045-3d005a73b01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_one(input_ids, entity_str, index):\n",
    "    \n",
    "    entity_id = tokenizer.encode(entity_str)\n",
    "    \n",
    "    if len(entity_id) != 3:\n",
    "        print(\"Not an appropriate entity!\")\n",
    "        return\n",
    "    \n",
    "    entity_id = entity_id[1]\n",
    "    \n",
    "    input_ids_list = input_ids.tolist()\n",
    "\n",
    "    all_entity_mention_indices = []\n",
    "    for i, j in enumerate(input_ids_list[0]):\n",
    "        if j == entity_id:\n",
    "            all_entity_mention_indices.append(i)\n",
    "    try:\n",
    "        entity_ind = all_entity_mention_indices[index]\n",
    "        return entity_ind\n",
    "    except:\n",
    "        print(\"entity not found in the input!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089439a2-7d15-4701-a8cd-bcbfb1a3c605",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question)\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 0) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0883d4-8795-4e9d-b9e7-99531865b1ae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = tm8\n",
    "improvements = []\n",
    "for context_previous in sentences:\n",
    "    \n",
    "    char = context_previous.split()[0]\n",
    "    answer = context_previous.split()[-1][:-1]\n",
    "    question = \"Where did \" + char + \" travel to?\"\n",
    "    entity_hidden_state = previous_timestep(context_previous, \"travelled\")\n",
    "    print(char)\n",
    "    print(answer)\n",
    "    print(question)\n",
    "    \n",
    "    context_current = \"Lucas was 30 years old.\"\n",
    "    probability, scores = current_timestep_regular(context_current, question, answer, template)\n",
    "    \n",
    "    context_enhanced = \" travelled \" + context_current\n",
    "    print(context_enhanced)\n",
    "    probability_enhanced, scores_enhanced = current_timestep_enhanced(context_enhanced, \n",
    "                                                                      question, \n",
    "                                                                      answer, \n",
    "                                                                      template, \n",
    "                                                                      \"travelled\", \n",
    "                                                                      entity_hidden_state)\n",
    "    \n",
    "    improvement = probability_enhanced-probability\n",
    "    print(improvement)\n",
    "    improvements.append( improvement )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "4ed94a57-2c85-4080-801d-259dd0c07107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.305687400116832e-05"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(improvements)/len(improvements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939df0d1-b9b2-4c0a-8948-e2f14dc7f4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"travelled\"] = improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43972cd1-79be-4285-80cd-7b7d0dfc4a02",
   "metadata": {},
   "source": [
    "# Open-Ended Generation - Where did he travel to? - \"to\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6407874-4efa-4f33-a99b-f354b81fac8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_one(input_ids, entity_str, index):\n",
    "    \n",
    "    entity_id = tokenizer.encode(entity_str)\n",
    "    \n",
    "    if len(entity_id) != 2:\n",
    "        print(\"Not an appropriate entity!\")\n",
    "        return\n",
    "    \n",
    "    entity_id = entity_id[0]\n",
    "    \n",
    "    input_ids_list = input_ids.tolist()\n",
    "\n",
    "    all_entity_mention_indices = []\n",
    "    for i, j in enumerate(input_ids_list[0]):\n",
    "        if j == entity_id:\n",
    "            all_entity_mention_indices.append(i)\n",
    "    try:\n",
    "        entity_ind = all_entity_mention_indices[index]\n",
    "        return entity_ind\n",
    "    except:\n",
    "        print(\"entity not found in the input!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da693ea3-4e38-4e07-967a-ef6f1ad30c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question)\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 1) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95883d56-a6e8-4b90-b03a-a64ad8110c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = tm8\n",
    "improvements = []\n",
    "for context_previous in sentences:\n",
    "    \n",
    "    char = context_previous.split()[0]\n",
    "    answer = context_previous.split()[-1][:-1]\n",
    "    question = \"Where did \" + char + \" travel to?\"\n",
    "    entity_hidden_state = previous_timestep(context_previous, \"to\")\n",
    "    print(char)\n",
    "    print(answer)\n",
    "    print(question)\n",
    "    \n",
    "    context_current = \"Lucas was 30 years old.\"\n",
    "    probability, scores = current_timestep_regular(context_current, question, answer, template)\n",
    "    \n",
    "    context_enhanced = \" to \" + context_current\n",
    "    print(context_enhanced)\n",
    "    probability_enhanced, scores_enhanced = current_timestep_enhanced(context_enhanced, \n",
    "                                                                      question, \n",
    "                                                                      answer, \n",
    "                                                                      template, \n",
    "                                                                      \"to\", \n",
    "                                                                      entity_hidden_state)\n",
    "    \n",
    "    improvement = probability_enhanced-probability\n",
    "    print(improvement)\n",
    "    improvements.append( improvement )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4962418-563c-4fc4-8fa2-a3f1f416ed05",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"to\"] = improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719ddcee-98cf-47fc-8a8b-8088fb6b4ecd",
   "metadata": {},
   "source": [
    "# Open-Ended Generation - Where did he travel to? - cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4027196b-4f8a-42c8-923d-c44a3665cd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_index_one(input_ids, entity_str, index):\n",
    "    \n",
    "    entity_id = tokenizer.encode(entity_str)\n",
    "    \n",
    "    if len(entity_id) != 2:\n",
    "        print(\"Not an appropriate entity!\")\n",
    "        return\n",
    "    \n",
    "    entity_id = entity_id[0]\n",
    "    \n",
    "    input_ids_list = input_ids.tolist()\n",
    "\n",
    "    all_entity_mention_indices = []\n",
    "    for i, j in enumerate(input_ids_list[0]):\n",
    "        if j == entity_id:\n",
    "            all_entity_mention_indices.append(i)\n",
    "    try:\n",
    "        entity_ind = all_entity_mention_indices[index]\n",
    "        return entity_ind\n",
    "    except:\n",
    "        print(\"entity not found in the input!\")\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb90616f-7035-4be2-a580-86790ce101df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question)\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 0) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f85c17-fecc-4205-adb4-d3b05e8710d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = tm8\n",
    "improvements = []\n",
    "for context_previous in sentences:\n",
    "    \n",
    "    char = context_previous.split()[0]\n",
    "    answer = context_previous.split()[-1][:-1]\n",
    "    question = \"Where did \" + char + \" travel to?\"\n",
    "    entity_hidden_state = previous_timestep(context_previous, answer)\n",
    "    print(char)\n",
    "    print(answer)\n",
    "    print(question)\n",
    "    \n",
    "    context_current = \"Lucas was 30 years old.\"\n",
    "    probability, scores = current_timestep_regular(context_current, question, answer, template)\n",
    "    \n",
    "    context_enhanced = \" to \" + context_current\n",
    "    print(context_enhanced)\n",
    "    probability_enhanced, scores_enhanced = current_timestep_enhanced(context_enhanced, \n",
    "                                                                      question, \n",
    "                                                                      answer, \n",
    "                                                                      template, \n",
    "                                                                      answer, \n",
    "                                                                      entity_hidden_state)\n",
    "    \n",
    "    improvement = probability_enhanced-probability\n",
    "    print(improvement)\n",
    "    improvements.append( improvement )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a9d32a-3fff-4457-a460-4b2442452a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dict[\"cities\"] = improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23789376-46b2-4d4b-a02e-164a31148e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/kuacc/users/bozyurt20/hpc_run/predictions/holmes_results.txt\", \"wb\") as f:\n",
    "    pickle.dump(results_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f7f17301-9caa-4a1c-810b-ea94bd2556bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['chars', 'travelled', 'to', 'cities'])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39291f33-e973-4133-a730-1ba14bb7408d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/kuacc/users/bozyurt20/hpc_run/predictions/holmes_results.txt\", \"rb\") as f:\n",
    "    results_dict = pickle.load( f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a76c9d53-52f4-44ba-8269-9a1d51fb1612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chars -0.008961449623581218\n",
      "travelled -0.005377411903955362\n",
      "to -0.0040957831087871455\n",
      "cities 0.9195870674045545\n"
     ]
    }
   ],
   "source": [
    "for key in results_dict:\n",
    "    value = results_dict[key]\n",
    "    print(key, sum(value) / len(value))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07b3964-a757-4fe8-a64c-1bf557d9b228",
   "metadata": {},
   "source": [
    "# Multiple Choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501b208b-ca92-43f7-a424-5da05565d0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question)\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 0) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    probability = next_token_scores[0][tokenizer.encode(answer)[0]].item()\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "\n",
    "    return probability, scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "beb88182-6e0f-4bb3-bc7c-d40f2ab50e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_timestep_regular(context, question, answer_choices, answer, template):\n",
    "    \n",
    "    prompt = template.render(context=context, question=question, answer_choices=answer_choices)\n",
    "    prompt = prompt.replace('\"', '\" ')\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    out = model.generate(input_ids, max_new_tokens=1, return_dict_in_generate=True, output_scores=True)\n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    loc_tokens = []\n",
    "    for location in answer_choices:\n",
    "        loc_tokens.append(tokenizer.encode(location)[0])\n",
    "\n",
    "    probabilities = {}\n",
    "    for loc_token in loc_tokens:\n",
    "        probabilities[tokenizer.decode(loc_token)] = next_token_scores[0][loc_token].item()\n",
    "\n",
    "    the_sum = sum(probabilities.values())\n",
    "    for key, value in probabilities.items():\n",
    "        probabilities[key] = value/the_sum\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "    \n",
    "    answer_probability = probabilities[answer]\n",
    "\n",
    "    return probabilities, scores, answer_probability\n",
    "\n",
    "def previous_timestep(context, entity):\n",
    "    \n",
    "    input_ids = tokenizer.encode(context, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    out = model.encoder(input_ids, output_special=True, output_hidden_states=True)\n",
    "    special_hidden = out.special_hidden_states # 24 x (1, T, d)\n",
    "\n",
    "    special_reformatted = torch.zeros(num_layers, len_input_ids, d_model) # (24, T, d)\n",
    "    for i, hidden in enumerate(special_hidden):\n",
    "        special_reformatted[i:i+1, :, :] = hidden\n",
    "    \n",
    "    entity_ind = find_index_one(input_ids, entity, 0)\n",
    "    #print(\"entity_ind:\" , entity_ind)\n",
    "    entity_hidden_state = special_reformatted[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    return entity_hidden_state\n",
    "\n",
    "def current_timestep_enhanced(context, question, answer_choices, answer, template, entity, entity_hidden_state):\n",
    "    prompt = template.render(context=context, question=question, answer_choices=answer_choices)\n",
    "    prompt = prompt.replace('\"', '\" ')\n",
    "    print(prompt)\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    entity_inds = [ find_index_one(input_ids, entity, 0) ]\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids,\n",
    "        entity_hidden_states=entity_hidden_state,\n",
    "        entity_inds=entity_inds,\n",
    "        max_new_tokens=1,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True\n",
    "    ) \n",
    "    next_token_scores = torch.nn.functional.softmax(\n",
    "                    out.scores[0], dim=-1\n",
    "                )  # (batch_size * num_beams, vocab_size)\n",
    "\n",
    "    loc_tokens = []\n",
    "    for location in answer_choices:\n",
    "        loc_tokens.append(tokenizer.encode(location)[0])\n",
    "\n",
    "    probabilities = {}\n",
    "    for loc_token in loc_tokens:\n",
    "        probabilities[tokenizer.decode(loc_token)] = next_token_scores[0][loc_token].item()\n",
    "\n",
    "    the_sum = sum(probabilities.values())\n",
    "    for key, value in probabilities.items():\n",
    "        probabilities[key] = value/the_sum\n",
    "\n",
    "    scores = []\n",
    "    for i, score in enumerate(next_token_scores[0]):\n",
    "        scores.append( (i, score.item()) )\n",
    "\n",
    "    scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    scores = [(tokenizer.decode(a), b) for a, b in scores]\n",
    "    \n",
    "    answer_probability = probabilities[answer]\n",
    "\n",
    "    return probabilities, scores, answer_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bb32dc42-0b58-4a7a-978b-66d3d727966d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-modeling_t5\n",
      "John\n",
      "London\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Oslo\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Oslo\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05038603603111219\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Paris\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Oslo\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Oslo\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.0036001993575111024\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Oslo\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Seoul\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Seoul\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.18845849458622466\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Istanbul\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Cairo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Cairo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.018103718173271985\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Beijing\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.051810425463975074\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Sydney\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Paris\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Paris\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.012612299350586442\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Cairo\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.08744674970918598\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Seoul\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Sydney\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Sydney\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.025946011785003657\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Rome\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Cairo\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - Cairo\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.0127052195908939\n",
      "new code working-modeling_t5\n",
      "John\n",
      "Prague\n",
      "Where was John?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - London\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was John?\n",
      "Options:\n",
      " - London\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.020235055228028126\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "London\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.23074792962691287\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Paris\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Seoul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Seoul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.24590400401780055\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Oslo\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Prague\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Prague\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.15905977969845042\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Istanbul\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Seoul\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Seoul\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.37522153043421813\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Beijing\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.16754573434479353\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Sydney\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.031421133653629196\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Cairo\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Oslo\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Oslo\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.06355767830582482\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Seoul\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - London\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - London\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.006060410777149625\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Rome\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.010315778432341327\n",
      "new code working-modeling_t5\n",
      "Harry\n",
      "Prague\n",
      "Where was Harry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - London\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Harry?\n",
      "Options:\n",
      " - London\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.012719236835134672\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "London\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Sydney\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Sydney\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.13152797077330458\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Paris\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0816942653658177\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Oslo\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0669004346912842\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Istanbul\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Paris\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Paris\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.03177850207864022\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Beijing\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.13860674071842516\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Sydney\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Cairo\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Cairo\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.025164936685867445\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Cairo\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Rome\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Rome\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04158801698617132\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Seoul\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Oslo\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Oslo\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0001482590743311918\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Rome\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Cairo\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Cairo\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0008545591964125576\n",
      "new code working-modeling_t5\n",
      "Andrew\n",
      "Prague\n",
      "Where was Andrew?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Paris\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Andrew?\n",
      "Options:\n",
      " - Paris\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.015455207754507386\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "London\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.031706025556913764\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Paris\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - London\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - London\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0619287007627396\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Oslo\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Beijing\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Beijing\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0539539232143767\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Istanbul\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05516461820656318\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Beijing\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.08756287074505664\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Sydney\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Paris\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Paris\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.10540348763859825\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Cairo\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Sydney\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Sydney\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.01413826934594914\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Seoul\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Sydney\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Sydney\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.021505511976000584\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Rome\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Paris\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Paris\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.0090170916522532\n",
      "new code working-modeling_t5\n",
      "Lisa\n",
      "Prague\n",
      "Where was Lisa?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Lisa?\n",
      "Options:\n",
      " - Rome\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.02055621205724023\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "London\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.12439381308141723\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Paris\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Prague\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Prague\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04540321573564926\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Oslo\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - London\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - London\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.03253440830001392\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Istanbul\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Sydney\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Sydney\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04268974939951231\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Beijing\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Cairo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Cairo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.034921257205950136\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Sydney\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Seoul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Seoul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.06902094385194013\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Cairo\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - London\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - London\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.0066942587926118735\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Seoul\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Prague\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Prague\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0007147196963979696\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Rome\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Sydney\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Sydney\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.040663498551606336\n",
      "new code working-modeling_t5\n",
      "Mary\n",
      "Prague\n",
      "Where was Mary?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Cairo\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Mary?\n",
      "Options:\n",
      " - Cairo\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.004875109164417801\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "London\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Sydney\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Sydney\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.10815861776823801\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Paris\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.003482274265415697\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Oslo\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.11414117186511503\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Istanbul\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04409617223384679\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Beijing\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.036235576485786083\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Sydney\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Seoul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Seoul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05041667643268799\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Cairo\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Beijing\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Beijing\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.012446418238222345\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Seoul\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Beijing\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Beijing\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.06275429857810971\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Rome\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Cairo\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.022521836513615202\n",
      "new code working-modeling_t5\n",
      "Henry\n",
      "Prague\n",
      "Where was Henry?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Beijing\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Henry?\n",
      "Options:\n",
      " - Beijing\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.009369626684572266\n",
      "new code working-modeling_t5\n",
      "David\n",
      "London\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.27161087348055385\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Paris\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.08236476909690338\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Oslo\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - London\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - London\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05727151379258072\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Istanbul\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Cairo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Cairo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0379239438159087\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Beijing\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Rome\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Rome\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.13613419477761113\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Sydney\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Beijing\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Beijing\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.04313541455261782\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Cairo\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Paris\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Paris\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04768267747717536\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Seoul\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Prague\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Prague\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.11902972744303497\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Rome\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Sydney\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Sydney\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.09112520225815701\n",
      "new code working-modeling_t5\n",
      "David\n",
      "Prague\n",
      "Where was David?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Beijing\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was David?\n",
      "Options:\n",
      " - Beijing\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.020186578851708645\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "London\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Seoul\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.15382501451818725\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Paris\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Prague\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Prague\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.1321666477971981\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Oslo\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Seoul\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Seoul\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.18090134677646774\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Istanbul\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Sydney\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Sydney\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04061187429218455\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Beijing\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.014295103869328363\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Sydney\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Rome\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Rome\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.16422036136743468\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Cairo\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Rome\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Rome\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.014632552341960037\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Seoul\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Rome\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Rome\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.07106493880754375\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Rome\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Prague\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Prague\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.14034525806455095\n",
      "new code working-modeling_t5\n",
      "Sophia\n",
      "Prague\n",
      "Where was Sophia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Cairo\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Sophia?\n",
      "Options:\n",
      " - Cairo\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.004773504490258887\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "London\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Cairo\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Cairo\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.10811882806637264\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Paris\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05355607144227445\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Oslo\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.03444759479599513\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Istanbul\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.023049510469876397\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Beijing\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Paris\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Paris\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.06248394311884853\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Sydney\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Seoul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Seoul\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.08788388449288542\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Cairo\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.014225849313107886\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Seoul\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Oslo\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Oslo\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.08926515814967714\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Rome\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - Istanbul\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.07352905399286669\n",
      "new code working-modeling_t5\n",
      "Olivia\n",
      "Prague\n",
      "Where was Olivia?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Olivia?\n",
      "Options:\n",
      " - London\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.005936448703458115\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "London\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Cairo\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Cairo\n",
      " - London\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.12334704606819798\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Paris\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Seoul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Seoul\n",
      " - Paris\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.04336014119940501\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Oslo\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Prague\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Prague\n",
      " - Oslo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.0031368304335577246\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Istanbul\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Oslo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Oslo\n",
      " - Istanbul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.06620109957485881\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Beijing\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Oslo\n",
      " - Beijing\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.05636108885807595\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Sydney\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - London\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - London\n",
      " - Sydney\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.07353493493871588\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Cairo\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Oslo\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Oslo\n",
      " - Cairo\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "0.1022015742944109\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Seoul\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Sydney\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Sydney\n",
      " - Seoul\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.010480182973865632\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Rome\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Beijing\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Beijing\n",
      " - Rome\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.056412598650629295\n",
      "new code working-modeling_t5\n",
      "Emma\n",
      "Prague\n",
      "Where was Emma?\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Paris\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "John Lucas was 30 years old.\n",
      "Read the following context and choose the best option to answer the question.\n",
      "Context: John Lucas was 30 years old.\n",
      "Question: Where was Emma?\n",
      "Options:\n",
      " - Paris\n",
      " - Prague\n",
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n",
      "-0.03890408369850727\n"
     ]
    }
   ],
   "source": [
    "template = tm1\n",
    "mc_improvements = []\n",
    "for context_previous in sentences:\n",
    "    \n",
    "    char = context_previous.split()[0]\n",
    "    answer = context_previous.split()[-1][:-1]\n",
    "    wrong_answer = random.choice(cities)\n",
    "    while wrong_answer == answer:\n",
    "        wrong_answer = random.choice(cities)\n",
    "    \n",
    "    answer_choices = [wrong_answer, answer]\n",
    "    \n",
    "    question = \"Where was \" + char + \"?\"\n",
    "    \n",
    "    entity_hidden_state = previous_timestep(context_previous, char)\n",
    "    print(char)\n",
    "    print(answer)\n",
    "    print(question)\n",
    "    \n",
    "    context_current = \"Lucas was 30 years old.\"\n",
    "    probabilities, scores, answer_probability = current_timestep_regular(context_current, question, answer_choices, answer, template)\n",
    "    \n",
    "    context_enhanced = char_previous + \" \" + context_current\n",
    "    print(context_enhanced)\n",
    "    probabilities_enhanced, scores_enhanced, answer_probability_enhanced = current_timestep_enhanced(context_enhanced, \n",
    "                                                                      question, \n",
    "                                                                      answer_choices,\n",
    "                                                                      answer, \n",
    "                                                                      template, \n",
    "                                                                      char, \n",
    "                                                                      entity_hidden_state)\n",
    "    \n",
    "    improvement = answer_probability_enhanced - answer_probability\n",
    "    print(improvement)\n",
    "    mc_improvements.append( improvement )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4fd39cee-93a5-4b4d-8f51-e2d46bfd1db4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.04816384322775649"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(mc_improvements)/len(mc_improvements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5059735a-ecc9-47bb-bb80-d78e33b5181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = tm1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "453302ef-96f2-4e6b-a18e-2650d73f1d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John travelled to Oslo.\n",
      "Where was John?\n",
      "Oslo\n",
      "John\n"
     ]
    }
   ],
   "source": [
    "context_previous = random.choice(sentences)\n",
    "print(context_previous)\n",
    "char_previous = context_previous.split()[0]\n",
    "answer_previous = context_previous.split()[-1][:-1]\n",
    "question_previous = \"Where was \" + char_previous + \"?\"\n",
    "print(question_previous)\n",
    "print(answer_previous)\n",
    "print(char_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "297f39c6-bf1a-407e-a2eb-91315cd591d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emma travelled to London.\n",
      "Where was John?\n",
      "Oslo\n",
      "Emma\n"
     ]
    }
   ],
   "source": [
    "context = random.choice(sentences)\n",
    "print(context)\n",
    "char = context.split()[0]\n",
    "answer = context.split()[-1][:-1]\n",
    "question = \"Where was \" + char_previous + \"?\"\n",
    "print(question)\n",
    "print(answer_previous)\n",
    "print(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d306a346-fdbb-46e2-b0e2-f12b60f52928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-modeling_t5\n",
      "entity_ind: 0\n"
     ]
    }
   ],
   "source": [
    "entity_hidden_state = previous_timestep(context_previous, char_previous)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ee950416-afd3-4516-ba61-d30b0119f2bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n"
     ]
    }
   ],
   "source": [
    "probability, scores = current_timestep_regular(context, question, answer_previous, template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2c4b31c2-bba8-4a0c-bb57-8197eb5f3bab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Emma travelled to London.\n"
     ]
    }
   ],
   "source": [
    "context = char_previous + \" \" + context\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "132b80a5-ae68-4c7e-83c5-e1bd3e7c2a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new code working-utils\n",
      "new code working-modeling_t5\n",
      "llama generation happening.\n",
      "new code working-modeling_t5\n"
     ]
    }
   ],
   "source": [
    "probability_enhanced, scores_enhanced = current_timestep_enhanced(context, question, answer_previous, template, char_previous, entity_hidden_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "3be9d312-eaec-4150-b6df-64854bc3e09b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.429386924835853e-05"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "7044ba6d-4b09-435e-bf31-3943e85c8637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2664374480664264e-05"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probability_enhanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "c0955b51-9eb1-4730-b69a-163e4e626854",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'John Emma travelled to London.'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d8a961fa-ce42-42b7-9b60-e7a43b0ec847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'London'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ded1fc-568b-425c-a96c-19038058c51f",
   "metadata": {},
   "source": [
    "# Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed983efa-741f-4f44-881f-52131c23b492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple Choice\n",
    "\n",
    "tm1 = Template(\"\"\"Read the following context and choose the best option to answer the question.\n",
    "Context: {{ context }}\n",
    "Question: {{ question }}\n",
    "Options:\n",
    " - {{ answer_choices | join(\"\\n - \") }}\"\"\")\n",
    "\n",
    "tm2 = Template(\"\"\"{{ context }}\n",
    "{{ question }}\n",
    "- {{ answer_choices | join(\"\\n- \") }}\"\"\")\n",
    "\n",
    "tm3 = Template(\"\"\"{{ context }}\n",
    "{{ question }}\n",
    "Pick the correct answer from the following options:\n",
    "- {{ answer_choices | join(\"\\n- \") }}\"\"\")\n",
    "\n",
    "tm4 = Template(\"\"\"{{ context }}\n",
    "According to the above context, choose the best option to answer the following question.\n",
    "Question: {{ question }}\n",
    "Options:\n",
    "- {{answer_choices | join(\"\\n - \")}}\n",
    "\"\"\")\n",
    "\n",
    "tm5 = Template(\"\"\"{{ context }}\n",
    "{{ question }}\n",
    "Pick the best answer from the following options:\n",
    "A. {{ answer0 }}\n",
    "B. {{ answer1 }}\n",
    "C. {{ answer2 }}\n",
    "D. {{ answer3 }}\"\"\")\n",
    "\n",
    "tm6 = Template(\"\"\"{{ context }}\n",
    "According to the above context, choose the best option to answer the following question.\n",
    "Question: {{ question }}\n",
    "Options:\n",
    "A. {{ answer0 }}\n",
    "B. {{ answer1 }}\n",
    "C. {{ answer2 }}\n",
    "D. {{ answer3 }}\"\"\")\n",
    "\n",
    "tm7 = Template(\"\"\"{{ context }}\n",
    "{{ question }}\n",
    "A. {{ answer0 }}\n",
    "B. {{ answer1 }}\n",
    "C. {{ answer2 }}\n",
    "D. {{ answer3 }}\"\"\")\n",
    "\n",
    "# Open-Ended\n",
    "\n",
    "tm8 = Template(\"\"\"Question: \"{{question}}\"\n",
    "Context: \"{{context}}\"\n",
    "Answer:\"\"\")\n",
    "\n",
    "tm9 = Template(\"\"\"{{ context }}\n",
    "Given the paragraph above, please answer correctly the following\n",
    "question:\n",
    "{{ question }}\"\"\")\n",
    "\n",
    "tm10 = Template(\"\"\"Given the following passage\n",
    "\"{{context}}\",\n",
    "answer the following question. Note that the answer is present within\n",
    "the text.\n",
    "Question: {{question}}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f773e-a9aa-46ab-9a63-2e849e5959f8",
   "metadata": {},
   "source": [
    "# Toy Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e1d0b93-755d-4b07-ab99-8243f430b86b",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_contexts = [\n",
    "    \"John went to London. Mary went to Paris. John loves Mary.\",\n",
    "    \n",
    "]\n",
    "\n",
    "current_contexts = [\n",
    "    \"David went to Beijing. Henry went to Sydney.\",\n",
    "    \n",
    "]\n",
    "\n",
    "questions = [\n",
    "    \"Where did John go?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21b4f196-9158-498e-bf9c-af8b8996c42e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1079, 1]\n",
      "[8929, 1]\n",
      "[5954, 1]\n",
      "[11712, 1]\n",
      "[3790, 1]\n",
      "[7780, 1]\n",
      "[1955, 1]\n",
      "[30174, 1]\n",
      "[25051, 1]\n",
      "[15325, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = [\"John\", \"Harry\", \"Andrew\", \"Lisa\", \"Mary\", \"Henry\", \"David\", \"Sophia\", \"Olivia\", \"Emma\"]\n",
    "for name in names:\n",
    "    print(tokenizer.encode(\" \" + name))\n",
    "len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d39c33e8-f86d-44e8-9060-da0a7491ac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = [\"London\", \"Paris\", \"Oslo\", \"Istanbul\", \"Beijing\", \"Sydney\", \"Cairo\", \"Seoul\", \"Rome\", \"Prague\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "65335465-89b9-43fd-906c-78c34ad1c3b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1524, 5, 1]\n",
      "[1919, 5, 1]\n",
      "[29206, 5, 1]\n",
      "[20958, 5, 1]\n",
      "[14465, 5, 1]\n",
      "[7476, 5, 1]\n",
      "[28600, 5, 1]\n",
      "[28343, 5, 1]\n",
      "[7332, 5, 1]\n",
      "[23564, 5, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name in cities:\n",
    "    print(tokenizer.encode(name+\".\"))\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b93496f-fd69-4720-b220-771af4414423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1524, 1]\n",
      "[1919, 1]\n",
      "[29206, 1]\n",
      "[20958, 1]\n",
      "[14465, 1]\n",
      "[7476, 1]\n",
      "[28600, 1]\n",
      "[28343, 1]\n",
      "[7332, 1]\n",
      "[23564, 1]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for name in cities:\n",
    "    print(tokenizer.encode(name))\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c1891c-a421-42f1-9904-bc6e6af5bd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John travelled to London.', 'John travelled to Paris.', 'John travelled to Oslo.', 'John travelled to Istanbul.', 'John travelled to Beijing.', 'John travelled to Sydney.', 'John travelled to Cairo.', 'John travelled to Seoul.', 'John travelled to Rome.', 'John travelled to Prague.', 'Harry travelled to London.', 'Harry travelled to Paris.', 'Harry travelled to Oslo.', 'Harry travelled to Istanbul.', 'Harry travelled to Beijing.', 'Harry travelled to Sydney.', 'Harry travelled to Cairo.', 'Harry travelled to Seoul.', 'Harry travelled to Rome.', 'Harry travelled to Prague.', 'Andrew travelled to London.', 'Andrew travelled to Paris.', 'Andrew travelled to Oslo.', 'Andrew travelled to Istanbul.', 'Andrew travelled to Beijing.', 'Andrew travelled to Sydney.', 'Andrew travelled to Cairo.', 'Andrew travelled to Seoul.', 'Andrew travelled to Rome.', 'Andrew travelled to Prague.', 'Lisa travelled to London.', 'Lisa travelled to Paris.', 'Lisa travelled to Oslo.', 'Lisa travelled to Istanbul.', 'Lisa travelled to Beijing.', 'Lisa travelled to Sydney.', 'Lisa travelled to Cairo.', 'Lisa travelled to Seoul.', 'Lisa travelled to Rome.', 'Lisa travelled to Prague.', 'Mary travelled to London.', 'Mary travelled to Paris.', 'Mary travelled to Oslo.', 'Mary travelled to Istanbul.', 'Mary travelled to Beijing.', 'Mary travelled to Sydney.', 'Mary travelled to Cairo.', 'Mary travelled to Seoul.', 'Mary travelled to Rome.', 'Mary travelled to Prague.', 'Henry travelled to London.', 'Henry travelled to Paris.', 'Henry travelled to Oslo.', 'Henry travelled to Istanbul.', 'Henry travelled to Beijing.', 'Henry travelled to Sydney.', 'Henry travelled to Cairo.', 'Henry travelled to Seoul.', 'Henry travelled to Rome.', 'Henry travelled to Prague.', 'David travelled to London.', 'David travelled to Paris.', 'David travelled to Oslo.', 'David travelled to Istanbul.', 'David travelled to Beijing.', 'David travelled to Sydney.', 'David travelled to Cairo.', 'David travelled to Seoul.', 'David travelled to Rome.', 'David travelled to Prague.', 'Sophia travelled to London.', 'Sophia travelled to Paris.', 'Sophia travelled to Oslo.', 'Sophia travelled to Istanbul.', 'Sophia travelled to Beijing.', 'Sophia travelled to Sydney.', 'Sophia travelled to Cairo.', 'Sophia travelled to Seoul.', 'Sophia travelled to Rome.', 'Sophia travelled to Prague.', 'Olivia travelled to London.', 'Olivia travelled to Paris.', 'Olivia travelled to Oslo.', 'Olivia travelled to Istanbul.', 'Olivia travelled to Beijing.', 'Olivia travelled to Sydney.', 'Olivia travelled to Cairo.', 'Olivia travelled to Seoul.', 'Olivia travelled to Rome.', 'Olivia travelled to Prague.', 'Emma travelled to London.', 'Emma travelled to Paris.', 'Emma travelled to Oslo.', 'Emma travelled to Istanbul.', 'Emma travelled to Beijing.', 'Emma travelled to Sydney.', 'Emma travelled to Cairo.', 'Emma travelled to Seoul.', 'Emma travelled to Rome.', 'Emma travelled to Prague.']\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for name in names:\n",
    "    for city in cities:\n",
    "        sentences.append(name + \" travelled to \" + city + \".\")\n",
    "    \n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0e3560dd-650e-4849-83f4-498bd6696349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1111, 1]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"travel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2dfa4c7a-fbb3-41e5-ae5b-11b0c8a5d038",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[12, 58, 1]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"to?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b630f776-9608-4195-9c4f-00a71be38aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82201479-b41c-4ea9-970b-3c1b95471c47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9033fb0c-2e11-4b8c-a526-b98d4c441526",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (676971914.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[15], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    cp modeling_t5.py /kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/transformers/models/t5/\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/transformers/models/')\n",
    "cp modeling_t5.py /kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/transformers/models/t5/\n",
    "cp utils.py /kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/transformers/generation/\n",
    "cp modeling_outputs.py /kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/transformers/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa7522-5427-4a8e-92e0-1d34ef26e125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
