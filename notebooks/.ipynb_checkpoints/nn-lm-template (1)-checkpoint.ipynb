{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Neural Probabilistic Language Model\n",
    "\n",
    "Reference: Bengio, Y., Ducharme, R., Vincent, P., & Jauvin, C. (2003). A neural probabilistic language model. *Journal of machine learning research, 3*. (Feb), 1137-1155. ([PDF](http://www.jmlr.org/papers/v3/bengio03a.html), [Sample code](https://github.com/neubig/nn4nlp-code/blob/master/02-lm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CuArray{Float32,N} where N"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Knet, Base.Iterators, IterTools, LinearAlgebra, StatsBase, Test, Random, CUDA\n",
    "macro size(z, s); esc(:(@assert (size($z) == $s) string(summary($z),!=,$s))); end # for debugging\n",
    "Knet.array_type[] = CuArray{Float32}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set `datadir` to the location of ptb on your filesystem. You can find the ptb data in the\n",
    "https://github.com/neubig/nn4nlp-code repo\n",
    "[data](https://github.com/neubig/nn4nlp-code/tree/master/data) directory. The code below\n",
    "clones the nn4nlp-code repo using `git clone https://github.com/neubig/nn4nlp-code.git` if\n",
    "the data directory does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "const datadir = \"nn4nlp-code/data/ptb\"\n",
    "isdir(datadir) || run(`git clone https://github.com/neubig/nn4nlp-code.git`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Vocabulary\n",
    "\n",
    "In this part we are going to implement a `Vocab` type that will map words to unique integers. The fields of `Vocab` are:\n",
    "* w2i: A dictionary from word strings to integers.\n",
    "* i2w: An array mapping integers to word strings.\n",
    "* unk: The integer id of the unknown word token.\n",
    "* eos: The integer id of the end of sentence token.\n",
    "* tokenizer: The function used to tokenize sentence strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Vocab\n",
    "    w2i::Dict{String,Int}\n",
    "    i2w::Vector{String}\n",
    "    unk::Int\n",
    "    eos::Int\n",
    "    tokenizer\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vocab constructor\n",
    "\n",
    "Implement a constructor for the `Vocab` type. The constructor should take a file path as\n",
    "an argument and create a `Vocab` object with the most frequent words from that file and\n",
    "optionally unk and eos tokens. The keyword arguments are:\n",
    "\n",
    "* tokenizer: The function used to tokenize sentence strings.\n",
    "* vocabsize: Maximum number of words in the vocabulary.\n",
    "* mincount: Minimum count of words in the vocabulary.\n",
    "* unk, eos: unk and eos strings, should be part of the vocabulary unless set to nothing.\n",
    "\n",
    "You may find the following Julia functions useful: `Dict`, `eachline`, `split`, `get`,\n",
    "`delete!`, `sort!`, `keys`, `collect`, `push!`, `pushfirst!`, `findfirst`. You can take\n",
    "look at their documentation using e.g. `@doc eachline`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function Vocab(file::String; tokenizer=split, vocabsize=Inf, mincount=1, unk=\"<unk>\", eos=\"<s>\")\n",
    "    # Your code here\n",
    "    freq = Dict()\n",
    "    w2i = Dict()\n",
    "    i2w = []\n",
    "    if unk != nothing\n",
    "        push!(i2w, unk)\n",
    "        w2i[unk] = 1\n",
    "        freq[unk] = 1\n",
    "    end\n",
    "    if eos != nothing\n",
    "        push!(i2w, eos)\n",
    "        w2i[eos] = 2\n",
    "        freq[eos] = 1\n",
    "    end\n",
    "    f = open(file, \"r\")\n",
    "    for line in eachline(f)\n",
    "        tokens = tokenizer(line)\n",
    "        for token in tokens\n",
    "            freq[token] = get(freq, token, 0) + 1\n",
    "        end\n",
    "        \n",
    "    end\n",
    "    sorted_freq = sort(collect(freq), by=x->x[2], rev=true)\n",
    "    i = length(i2w) + 1\n",
    "    for tuple in sorted_freq\n",
    "        if tuple[2] >= mincount && tuple[1] != unk && tuple[1] != eos\n",
    "            w2i[tuple[1]] = i\n",
    "            push!(i2w, tuple[1])\n",
    "            i += 1\n",
    "        end\n",
    "        if length(w2i) == vocabsize\n",
    "            break\n",
    "        end\n",
    "    end         \n",
    "    return Vocab(w2i, i2w, w2i[unk], w2i[eos], tokenizer) \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Vocab\n",
      "└ @ Main In[7]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing Vocab\"\n",
    "f = \"$datadir/train.txt\"\n",
    "v = Vocab(f)\n",
    "@test all(v.w2i[w] == i for (i,w) in enumerate(v.i2w))\n",
    "@test length(Vocab(f).i2w) == 10000\n",
    "@test length(Vocab(f, vocabsize=1234).i2w) == 1234\n",
    "@test length(Vocab(f, mincount=5).i2w) == 9859"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the training data as our vocabulary source for the rest of the assignment. It\n",
    "has already been tokenized, lowercased, and words other than the most frequent 10000 have\n",
    "been replaced with `\"<unk>\"`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab(Dict(\"adviser\" => 1750,\"enjoy\" => 4607,\"advertisements\" => 7826,\"fight\" => 1441,\"nicholas\" => 3783,\"everywhere\" => 6278,\"surveyed\" => 3556,\"helping\" => 2081,\"whose\" => 621,\"manufacture\" => 5052…), [\"<unk>\", \"<s>\", \"the\", \"N\", \"of\", \"to\", \"a\", \"in\", \"and\", \"'s\"  …  \"cluett\", \"hydro-quebec\", \"memotec\", \"photography\", \"ipo\", \"ssangyong\", \"fromstein\", \"ferc\", \"gitano\", \"daewoo\"], 1, 2, split)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_vocab = Vocab(\"$datadir/train.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. TextReader\n",
    "\n",
    "Next we will implement `TextReader`, an iterator that reads sentences from a file and\n",
    "returns them as integer arrays using a `Vocab`.  We want to implement `TextReader` as an\n",
    "iterator for scalability. Instead of reading the whole file at once, `TextReader` will\n",
    "give us one sentence at a time as needed (similar to how `eachline` works). This will help\n",
    "us handle very large files in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct TextReader\n",
    "    file::String\n",
    "    vocab::Vocab\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### iterate\n",
    "\n",
    "The main function to implement for a new iterator is `iterate`. The `iterate` function\n",
    "takes an iterator and optionally a state, and returns a `(nextitem,state)` if the iterator\n",
    "has more items or `nothing` otherwise. A one argument call `iterate(x)` starts the\n",
    "iteration, and a two argument call `iterate(x,state)` continues from where it left off.\n",
    "\n",
    "Here are some sources you may find useful on iterators:\n",
    "\n",
    "* https://github.com/denizyuret/Knet.jl/blob/master/tutorial/25.iterators.ipynb\n",
    "* https://docs.julialang.org/en/v1/manual/interfaces\n",
    "* https://docs.julialang.org/en/v1/base/collections/#lib-collections-iteration-1\n",
    "* https://docs.julialang.org/en/v1/base/iterators\n",
    "* https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1\n",
    "* https://juliacollections.github.io/IterTools.jl/stable\n",
    "\n",
    "For `TextReader` the state should be an `IOStream` object obtained by `open(file)` at the\n",
    "start of the iteration. When `eof(state)` indicates that end of file is reached, the\n",
    "stream should be closed by `close(state)` and `nothing` should be returned. Otherwise\n",
    "`TextReader` reads the next line from the file using `readline`, tokenizes it, maps each\n",
    "word to its integer id using the vocabulary and returns the resulting integer array\n",
    "(without any eos tokens) and the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "function Base.iterate(r::TextReader, s=nothing)\n",
    "    # Your code here\n",
    "    if s == nothing        \n",
    "        s = open(r.file)\n",
    "    elseif eof(s) == true\n",
    "        close(s)\n",
    "        return nothing\n",
    "    end\n",
    "    line = readline(s)\n",
    "    tokens = r.vocab.tokenizer(line)\n",
    "    ids = []\n",
    "    for token in tokens\n",
    "        push!(ids, r.vocab.w2i[token])\n",
    "    end\n",
    "    return (ids, s)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some optional functions that can be defined for iterators. They are required for\n",
    "`collect` to work, which converts an iterator to a regular array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Base.IteratorSize(::Type{TextReader}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{TextReader}) = Base.HasEltype()\n",
    "Base.eltype(::Type{TextReader}) = Vector{Int}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing TextReader\n",
      "└ @ Main In[12]:1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@info \"Testing TextReader\"\n",
    "train_sentences, valid_sentences, test_sentences =\n",
    "    (TextReader(\"$datadir/$file.txt\", train_vocab) for file in (\"train\",\"valid\",\"test\"))\n",
    "@test length(first(train_sentences)) == 24\n",
    "@test length(collect(train_sentences)) == 42068\n",
    "@test length(collect(valid_sentences)) == 3370\n",
    "@test length(collect(test_sentences)) == 3761"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Model\n",
    "\n",
    "We are going to first implement some reusable layers for our model. Layers and models are\n",
    "basically functions with associated parameters. Please review [Function-like\n",
    "objects](https://docs.julialang.org/en/v1/manual/methods/#Function-like-objects-1) for how\n",
    "to best define such objects in Julia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embed\n",
    "\n",
    "`Embed` is a layer that takes an integer or an array of integers as input, uses them as\n",
    "column indices to lookup embeddings in its parameter matrix `w`, and returns these columns\n",
    "packed into an array. If the input size is `(X1,X2,...)`, the output size will be\n",
    "`(C,X1,X2,...)` where C is the columns size of `w` (which Julia will automagically\n",
    "accomplish if you use the right indexing expression). Please review [Array\n",
    "indexing](https://docs.julialang.org/en/v1/manual/arrays/#man-array-indexing-1) and the\n",
    "Knet `param` function to implement this layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Embed; w; end\n",
    "\n",
    "function Embed(vocabsize::Int, embedsize::Int)\n",
    "    # Your code here\n",
    "    Embed(param(embedsize, vocabsize))\n",
    "end\n",
    "\n",
    "function (l::Embed)(x)\n",
    "    # Your code here\n",
    "    l.w[:,x]\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Embed\n",
      "└ @ Main In[18]:1\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "CUDA error (code 999, CUDA_ERROR_UNKNOWN)",
     "output_type": "error",
     "traceback": [
      "CUDA error (code 999, CUDA_ERROR_UNKNOWN)",
      "",
      "Stacktrace:",
      " [1] throw_api_error(::CUDA.cudaError_enum) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/cudadrv/error.jl:97",
      " [2] macro expansion at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/cudadrv/error.jl:104 [inlined]",
      " [3] cuDevicePrimaryCtxRetain(::Base.RefValue{Ptr{Nothing}}, ::CuDevice) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/utils/call.jl:93",
      " [4] CuContext(::CuPrimaryContext) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/cudadrv/context/primary.jl:32",
      " [5] context(::CuDevice) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:258",
      " [6] device!(::CuDevice, ::Nothing) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:295",
      " [7] device!(::CuDevice) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:274",
      " [8] initialize_thread(::Int64) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:122",
      " [9] prepare_cuda_call() at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:80",
      " [10] device at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:227 [inlined]",
      " [11] alloc at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/pool.jl:293 [inlined]",
      " [12] CuArray{Float32,2}(::UndefInitializer, ::Tuple{Int64,Int64}) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/array.jl:20",
      " [13] CuArray at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/array.jl:190 [inlined]",
      " [14] CuArray{Float32,N} where N(::Array{Float64,2}) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/array.jl:196",
      " [15] param(::Int64, ::Vararg{Int64,N} where N; init::Function, atype::Type) at /home/bozyurt20/.julia/packages/Knet/C0PoK/src/train20/param.jl:23",
      " [16] param at /home/bozyurt20/.julia/packages/Knet/C0PoK/src/train20/param.jl:23 [inlined]",
      " [17] Embed(::Int64, ::Int64) at ./In[17]:5",
      " [18] top-level scope at In[18]:3",
      " [19] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "@info \"Testing Embed\"\n",
    "Random.seed!(1)\n",
    "embed = Embed(100,10)\n",
    "input = rand(1:100, 2, 3)\n",
    "output = embed(input)\n",
    "@test size(output) == (10, 2, 3)\n",
    "@test norm(output) ≈ 1.0392102f0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear\n",
    "\n",
    "The `Linear` layer implements an affine transformation of its input: `w*x .+ b`. `w`\n",
    "should be initialized with small random numbers and `b` with zeros. Please review `param`\n",
    "and `param0` functions from Knet for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Linear; w; b; end\n",
    "\n",
    "function Linear(inputsize::Int, outputsize::Int)\n",
    "    # Your code here\n",
    "    w = param(outputsize, inputsize)\n",
    "    b = param0(outputsize,)\n",
    "    Linear(w,b)\n",
    "end\n",
    "\n",
    "function (l::Linear)(x)\n",
    "    # Your code here\n",
    "    l.w * x .+ l.b\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Testing Linear\n",
      "└ @ Main In[23]:1\n"
     ]
    },
    {
     "ename": "LoadError",
     "evalue": "CUDA error (code 999, CUDA_ERROR_UNKNOWN)",
     "output_type": "error",
     "traceback": [
      "CUDA error (code 999, CUDA_ERROR_UNKNOWN)",
      "",
      "Stacktrace:",
      " [1] throw_api_error(::CUDA.cudaError_enum) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/cudadrv/error.jl:97",
      " [2] macro expansion at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/cudadrv/error.jl:104 [inlined]",
      " [3] cuDevicePrimaryCtxRetain(::Base.RefValue{Ptr{Nothing}}, ::CuDevice) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/utils/call.jl:93",
      " [4] CuContext(::CuPrimaryContext) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/lib/cudadrv/context/primary.jl:32",
      " [5] context(::CuDevice) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:258",
      " [6] device!(::CuDevice, ::Nothing) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:295",
      " [7] device!(::CuDevice) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:274",
      " [8] initialize_thread(::Int64) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:122",
      " [9] prepare_cuda_call() at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:80",
      " [10] device at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/state.jl:227 [inlined]",
      " [11] alloc at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/pool.jl:293 [inlined]",
      " [12] CuArray{Float32,2}(::UndefInitializer, ::Tuple{Int64,Int64}) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/array.jl:20",
      " [13] CuArray at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/array.jl:190 [inlined]",
      " [14] CuArray{Float32,N} where N(::Array{Float64,2}) at /home/bozyurt20/.julia/packages/CUDA/FlHUF/src/array.jl:196",
      " [15] param(::Int64, ::Vararg{Int64,N} where N; init::Function, atype::Type) at /home/bozyurt20/.julia/packages/Knet/C0PoK/src/train20/param.jl:23",
      " [16] param at /home/bozyurt20/.julia/packages/Knet/C0PoK/src/train20/param.jl:23 [inlined]",
      " [17] Linear(::Int64, ::Int64) at ./In[19]:5",
      " [18] top-level scope at In[23]:3",
      " [19] include_string(::Function, ::Module, ::String, ::String) at ./loading.jl:1091"
     ]
    }
   ],
   "source": [
    "@info \"Testing Linear\"\n",
    "Random.seed!(1)\n",
    "linear = Linear(100,10)\n",
    "input = oftype(linear.w, randn(Float32, 100, 5))\n",
    "output = linear(input)\n",
    "@test size(output) == (10, 5)\n",
    "@test norm(output) ≈ 9.578475f0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NNLM\n",
    "\n",
    "`NNLM` is the model object. It has the following fields:\n",
    "* vocab: The `Vocab` object associated with this model.\n",
    "* windowsize: How many words of history the model looks at (ngram order).\n",
    "* embed: An `Embed` layer.\n",
    "* hidden: A `Linear` layer which should be followed by `tanh.` to produce the hidden activations.\n",
    "* output: A `Linear` layer to map hidden activations to vocabulary scores.\n",
    "* dropout: A number between 0 and 1 indicating dropout probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct NNLM; vocab; windowsize; embed; hidden; output; dropout; end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The constructor for `NNLM` takes a vocabulary and various size parameters, returns an\n",
    "`NNLM` object. Remember that the embeddings for `windowsize` words will be concatenated\n",
    "before being fed to the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function NNLM(vocab::Vocab, windowsize::Int, embedsize::Int, hiddensize::Int, dropout::Real)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default model parameters\n",
    "HIST = 3\n",
    "EMBED = 128\n",
    "HIDDEN = 128\n",
    "DROPOUT = 0.5\n",
    "VOCAB = length(train_vocab.i2w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing NNLM\"\n",
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "@test model.vocab === train_vocab\n",
    "@test model.windowsize === HIST\n",
    "@test size(model.embed.w) == (EMBED,VOCAB)\n",
    "@test size(model.hidden.w) == (HIDDEN,HIST*EMBED)\n",
    "@test size(model.hidden.b) == (HIDDEN,)\n",
    "@test size(model.output.w) == (VOCAB,HIDDEN)\n",
    "@test size(model.output.b) == (VOCAB,)\n",
    "@test model.dropout == 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. One word at a time\n",
    "\n",
    "Conceptually the easiest way to implement the neural language model is by processing one\n",
    "word at a time. This is also computationally the most expensive, which we will address in\n",
    "upcoming parts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pred_v1\n",
    "\n",
    "`pred_v1` takes a model and a `windowsize` length vector of integer word ids indicating the\n",
    "current history, and returns a vocabulary sized vector of scores for the next word. The\n",
    "embeddings of the `windowsize` words are reshaped to a single vector before being fed to the\n",
    "hidden layer. The hidden output is passed through elementwise `tanh` before being fed to\n",
    "the output layer. Dropout is applied to embedding and hidden outputs.\n",
    "\n",
    "Please review Julia functions `vec`, `reshape`, `tanh`, and Knet function `dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function pred_v1(m::NNLM, hist::AbstractVector{Int})\n",
    "    @assert length(hist) == m.windowsize\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing pred_v1\"\n",
    "h = repeat([model.vocab.eos], model.windowsize)\n",
    "p = pred_v1(model, h)\n",
    "@test size(p) == size(train_vocab.i2w)\n",
    "\n",
    "\n",
    "# This predicts the scores for the whole sentence, will be used for later testing.\n",
    "function scores_v1(model, sent)\n",
    "    hist = repeat([ model.vocab.eos ], model.windowsize)\n",
    "    scores = []\n",
    "    for word in [ sent; model.vocab.eos ]\n",
    "        push!(scores, pred_v1(model, hist))\n",
    "        hist = [ hist[2:end]; word ]\n",
    "    end\n",
    "    hcat(scores...)\n",
    "end\n",
    "\n",
    "sent = first(train_sentences)\n",
    "@test size(scores_v1(model, sent)) == (length(train_vocab.i2w), length(sent)+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### generate\n",
    "\n",
    "`generate` takes a model `m` and generates a random sentence of maximum length\n",
    "`maxlength`. It initializes a history of `m.windowsize` `m.vocab.eos` tokens. Then it\n",
    "computes the scores for the next word using `pred_v1` and samples a next word using\n",
    "normalized exp of scores as probabilities. It pushes this next word into history and keeps\n",
    "going until `m.vocab.eos` is picked or `maxlength` is reached. It returns a sentence\n",
    "string consisting of concatenated word strings separated by spaces.\n",
    "\n",
    "Please review Julia functions `repeat`, `push!`, `join` and StatsBase function `sample`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate(m::NNLM; maxlength=30)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing generate\"\n",
    "s = generate(model, maxlength=5)\n",
    "@test s isa String\n",
    "@test length(split(s)) <= 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss_v1\n",
    "\n",
    "`loss_v1` computes the negative log likelihood loss given a model `m` and sentence `sent`\n",
    "using `pred_v1`. If `average=true` it returns the per-word average loss, if\n",
    "`average=false` it returns a `(total_loss, num_words)` pair. To compute the loss it starts\n",
    "with a history of `m.windowsize` `m.vocab.eos` tokens like `generate`. Then, for each word\n",
    "in `sent` and a final `eos` token, it computes the scores based on the history, converts\n",
    "them to negative log probabilities, adds the entry corresponding to the current word to\n",
    "the total loss and pushes the current word to history.\n",
    "\n",
    "Please review Julia functions `repeat`, `vcat` and Knet functions `logp`, `nll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_v1(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing loss_v1\"\n",
    "s = first(train_sentences)\n",
    "avgloss = loss_v1(model,s)\n",
    "(tot, cnt) = loss_v1(model, s, average = false)\n",
    "@test 9 < avgloss < 10\n",
    "@test cnt == length(s) + 1\n",
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### maploss\n",
    "\n",
    "`maploss` takes a loss function `lossfn`, a model `model` and a dataset `data` and returns\n",
    "the average per word negative log likelihood loss if `average=true` or `(total_loss,num_words)`\n",
    "if `average=false`. `data` may be an iterator over sentences (e.g. `TextReader`) or batches\n",
    "of sentences. Computing the loss over a whole dataset is useful to monitor our performance\n",
    "during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function maploss(lossfn, model, data; average = true)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing maploss\"\n",
    "tst100 = collect(take(test_sentences, 100))\n",
    "avgloss = maploss(loss_v1, model, tst100)\n",
    "@test 9 < avgloss < 10\n",
    "(tot, cnt) = maploss(loss_v1, model, tst100, average = false)\n",
    "@test cnt == length(tst100) + sum(length.(tst100))\n",
    "@test tot/cnt ≈ avgloss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing loss_v1\n",
    "\n",
    "Unfortunately processing data one word at a time is not very efficient. The following\n",
    "shows that we can only train about 40-50 sentences per second on a V100 GPU. The training\n",
    "data has 42068 sentences which would take about 1000 seconds or 15 minutes. We probably\n",
    "need 10-100 epochs for convergence which is getting too long for this assignment. Let's\n",
    "see if we can speed things up by processing more data in parallel.\n",
    "\n",
    "Please review Knet function `sgd!` used below as well as iterator functions `collect`,\n",
    "`take`, and [Generator\n",
    "expressions](https://docs.julialang.org/en/v1/manual/arrays/#Generator-Expressions-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Timing loss_v1 inference with 1000 sentences\"\n",
    "tst1000 = collect(take(test_sentences, 1000))\n",
    "GC.gc(true); @time maploss(loss_v1, model, tst1000)\n",
    "\n",
    "@info \"Timing loss_v1 training with 100 sentences\"\n",
    "trn100 = ((model,x) for x in collect(take(train_sentences, 100)))\n",
    "GC.gc(true); @time sgd!(loss_v1, trn100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. One sentence at a time\n",
    "\n",
    "We may have to do things one word at a time when generating a sentence, but there is no\n",
    "reason not to do things in parallel for loss calculation. In this part you will implement\n",
    "`pred_v2` and `loss_v2` which do calculations for the whole sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pred_v2\n",
    "\n",
    "`pred_v2` takes a model `m`, an N×S array of word ids `hist` and produces a V×S array of\n",
    "scores where N is `m.windowsize`, V is the vocabulary size and `S` is sentence length\n",
    "including the final eos token. The `hist` array has already been padded and shifted such\n",
    "that `hist[:,i]` is the N word context to predict word i. `pred_v2` starts by finding the\n",
    "embeddings for all hist entries at once, a E×N×S array where E is the embedding size. The\n",
    "N embeddings for each context are concatenated by reshaping this array to (E*N)×S. After a\n",
    "dropout step, the hidden layer converts this to an H×S array where H is the hidden\n",
    "size. Following a `tanh` and `dropout`, the output layer produces the final result as a\n",
    "V×S array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function pred_v2(m::NNLM, hist::AbstractMatrix{Int})\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing pred_v2\"\n",
    "\n",
    "function scores_v2(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    return pred_v2(model, hist)\n",
    "end\n",
    "\n",
    "sent = first(test_sentences)\n",
    "s1, s2 = scores_v1(model, sent), scores_v2(model, sent)\n",
    "@test size(s1) == size(s2) == (length(train_vocab.i2w), length(sent)+1)\n",
    "@test s1 ≈ s2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss_v2\n",
    "\n",
    "`loss_v2` computes the negative log likelihood loss given a model `m` and sentence `sent`\n",
    "using `pred_v2`. If `average=true` it returns the per-word average loss, if\n",
    "`average=false` it returns a `(total_loss, num_words)` pair. To compute the loss it\n",
    "constructs a N×S history matrix such that `hist[:,i]` gives the N word context to predict\n",
    "word i where N is `m.windowsize` and S is the sentence length + 1 for the final eos token.\n",
    "Then it computes the scores for all S tokens using `pred_v2`, converts them to negative\n",
    "log probabilities, computes the loss based on the entries for the correct words.\n",
    "\n",
    "Please review the Knet function `nll`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_v2(m::NNLM, sent::AbstractVector{Int}; average = true)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing loss_v2\"\n",
    "s = first(test_sentences)\n",
    "@test loss_v1(model, s) ≈ loss_v2(model, s)\n",
    "tst100 = collect(take(test_sentences, 100))\n",
    "@test maploss(loss_v1, model, tst100) ≈ maploss(loss_v2, model, tst100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing loss_v2\n",
    "\n",
    "The following tests show that loss_v2 works about 15-20 times faster than loss_v1 during\n",
    "maploss and training. We can train at 800+ sentences/second on a V100 GPU, which is under\n",
    "a minute per epoch. We could stop here and train a reasonable model, but let's see if we\n",
    "can squeeze a bit more performance by minibatching sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Timing loss_v2 inference with 10K sentences\"\n",
    "tst10k = collect(take(train_sentences, 10000))\n",
    "GC.gc(true); @time maploss(loss_v2, model, tst10k)\n",
    "\n",
    "@info \"Timing loss_v2 training with 1000 sentences\"\n",
    "trn1k = ((model,x) for x in collect(take(train_sentences, 1000)))\n",
    "GC.gc(true); @time sgd!(loss_v2, trn1k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6. Multiple sentences at a time (minibatching)\n",
    "\n",
    "To get even more performance out of a GPU we will process multiple sentences at a\n",
    "time. This is called minibatching and is unfortunately complicated by the fact that the\n",
    "sentences in a batch may not be of the same length. Let's first write the minibatched\n",
    "versions of `pred` and `loss`, and see how to batch sentences together later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pred_v3\n",
    "\n",
    "`pred_v3` takes a model `m`, a N×B×S dimensional history array `hist`, and returns a V×B×S\n",
    "dimensional score array, where N is `m.windowsize`, V is the vocabulary size, B is the batch\n",
    "size, and S is maximum sentence length in the batch + 1 for the final eos token. First,\n",
    "the embeddings for all entries in `hist` are looked up, which results in an array of\n",
    "E×N×B×S where E is the embedding size. The embedding array is reshaped to (E*N)×(B*S) and\n",
    "dropout is applied. It is then fed to the hidden layer which returns a H×(B*S) hidden\n",
    "output where H is the hidden size. Following element-wise tanh and dropout, the output\n",
    "layer turns this into a score array of V×(B*S) which is reshaped and returned as a V×B×S\n",
    "dimensional tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function pred_v3(m::NNLM, hist::Array{Int})\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing pred_v3\"\n",
    "\n",
    "function scores_v3(model, sent)\n",
    "    hist = [ repeat([ model.vocab.eos ], model.windowsize); sent ]\n",
    "    hist = vcat((hist[i:end+i-model.windowsize]' for i in 1:model.windowsize)...)\n",
    "    @assert size(hist) == (model.windowsize, length(sent)+1)\n",
    "    hist = reshape(hist, size(hist,1), 1, size(hist,2))\n",
    "    return pred_v3(model, hist)\n",
    "end\n",
    "\n",
    "sent = first(train_sentences)\n",
    "@test scores_v2(model, sent) ≈ scores_v3(model, sent)[:,1,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mask!\n",
    "\n",
    "`mask!` takes matrix `a` and a pad value `pad`. It replaces all but one of the pads at the\n",
    "end of each row with 0's. This can be used in `loss_v3` for the loss calculation: the Knet\n",
    "`nll` function skips 0's in the answer array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function mask!(a,pad)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing mask!\"\n",
    "a = [1 2 1 1 1; 2 2 2 1 1; 1 1 2 2 2; 1 1 2 2 1]\n",
    "@test mask!(a,1) == [1 2 1 0 0; 2 2 2 1 0; 1 1 2 2 2; 1 1 2 2 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss_v3\n",
    "\n",
    "`loss_v3` computes the negative log likelihood loss given a model `m` and sentence\n",
    "minibatch `batch` using `pred_v3`. If `average=true` it returns the per-word average loss,\n",
    "if `average=false` it returns a `(total_loss, num_words)` pair. The batch array has\n",
    "dimensions B×S where B is the batch size and S is the length of the longest sentence in\n",
    "the batch + 1 for the final eos token. Each row contains the word ids of a sentence padded\n",
    "with eos tokens on the right.  Sentences in a batch may have different lengths. `loss_v3`\n",
    "first constructs a history array of size N×B×S from the batch such that `hist[:,i,j]`\n",
    "gives the N word context to the j'th word of the i'th sentence. This is done by repeating,\n",
    "slicing, concatenating, reshaping and/or using permutedims on the batch array. Next\n",
    "`pred_v3` is used to compute the scores array of size V×B×S where V is the vocabulary\n",
    "size. The correct answers are extracted from the batch to an array of size B×S and the\n",
    "extra padding at the end of each sentence (after the final eos) is masked (extra eos\n",
    "replaced by zeros).  Finally the scores and the masked correct answers are used to compute\n",
    "the negative log likelihood loss using `nll`.\n",
    "\n",
    "Please review array slicing, Julia functions `vcat`, `hcat`, `reshape`, `permutedims`, and\n",
    "the Knet function `nll` for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_v3(m::NNLM, batch::AbstractMatrix{Int}; average = true)\n",
    "    # Your code here\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Testing loss_v3\"\n",
    "s = first(test_sentences)\n",
    "b = [ s; model.vocab.eos ]'\n",
    "@test loss_v2(model, s) ≈ loss_v3(model, b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minibatching\n",
    "\n",
    "Below is a sample implementation of a sequence minibatcher. The `LMData` iterator wraps a\n",
    "TextReader and produces batches of sentences with similar length to minimize padding (too\n",
    "much padding wastes computation). To be able to scale to very large files, we do not want\n",
    "to read the whole file, sort by length etc. Instead `LMData` keeps around a small number\n",
    "of buckets and fills them with similar sized sentences from the TextReader. As soon as one\n",
    "of the buckets reaches the desired batch size it is turned into a matrix with the\n",
    "necessary padding and output. When the TextReader is exhausted the remaining buckets are\n",
    "returned (which may have smaller batch sizes). I will let you figure the rest out from the\n",
    "following, there is no code to write for this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct LMData\n",
    "    src::TextReader\n",
    "    batchsize::Int\n",
    "    maxlength::Int\n",
    "    bucketwidth::Int\n",
    "    buckets\n",
    "end\n",
    "\n",
    "function LMData(src::TextReader; batchsize = 64, maxlength = typemax(Int), bucketwidth = 10)\n",
    "    numbuckets = min(128, maxlength ÷ bucketwidth)\n",
    "    buckets = [ [] for i in 1:numbuckets ]\n",
    "    LMData(src, batchsize, maxlength, bucketwidth, buckets)\n",
    "end\n",
    "\n",
    "Base.IteratorSize(::Type{LMData}) = Base.SizeUnknown()\n",
    "Base.IteratorEltype(::Type{LMData}) = Base.HasEltype()\n",
    "Base.eltype(::Type{LMData}) = Matrix{Int}\n",
    "\n",
    "function Base.iterate(d::LMData, state=nothing)\n",
    "    if state == nothing\n",
    "        for b in d.buckets; empty!(b); end\n",
    "    end\n",
    "    bucket,ibucket = nothing,nothing\n",
    "    while true\n",
    "        iter = (state === nothing ? iterate(d.src) : iterate(d.src, state))\n",
    "        if iter === nothing\n",
    "            ibucket = findfirst(x -> !isempty(x), d.buckets)\n",
    "            bucket = (ibucket === nothing ? nothing : d.buckets[ibucket])\n",
    "            break\n",
    "        else\n",
    "            sent, state = iter\n",
    "            if length(sent) > d.maxlength || length(sent) == 0; continue; end\n",
    "            ibucket = min(1 + (length(sent)-1) ÷ d.bucketwidth, length(d.buckets))\n",
    "            bucket = d.buckets[ibucket]\n",
    "            push!(bucket, sent)\n",
    "            if length(bucket) === d.batchsize; break; end\n",
    "        end\n",
    "    end\n",
    "    if bucket === nothing; return nothing; end\n",
    "    batchsize = length(bucket)\n",
    "    maxlen = maximum(length.(bucket))\n",
    "    batch = fill(d.src.vocab.eos, batchsize, maxlen + 1)\n",
    "    for i in 1:batchsize\n",
    "        batch[i, 1:length(bucket[i])] = bucket[i]\n",
    "    end\n",
    "    empty!(bucket)\n",
    "    return batch, state\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Timing loss_v3\n",
    "\n",
    "We can compare the speeds of `loss_v2` and `loss_v3` using various batch sizes. Running\n",
    "the following on a V100 suggests that for forward loss calculation, a batchsize around 16\n",
    "gives the best speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Timing inference for loss_v2 and loss_v3 at various batch sizes\"\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(true); @time p2 = maploss(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches_B = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(true); @time p3 = maploss(loss_v3, model, test_batches_B); @test p3 ≈ p2\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, a batchsize around 64 seems best, although things are a bit more complicated\n",
    "here: larger batch sizes make fewer updates per epoch which may slow down convergence. We\n",
    "will use the smaller test data to get quick results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Timing training for loss_v2 and loss_v3 at various batch sizes\"\n",
    "train(loss, model, data) = sgd!(loss, ((model,sent) for sent in data))\n",
    "@info loss_v2; test_collect = collect(test_sentences)\n",
    "GC.gc(true); @time train(loss_v2, model, test_collect)\n",
    "for B in (1, 8, 16, 32, 64, 128, 256)\n",
    "    @info loss_v3,B; test_batches_B = collect(LMData(test_sentences, batchsize = B))\n",
    "    GC.gc(true); @time train(loss_v3, model, test_batches_B)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 7. Training\n",
    "\n",
    "You should be able to get the validation loss under 5.25 (perplexity under 190) in 100\n",
    "epochs with default parameters.  This takes about 5 minutes on a V100 or T4 GPU.\n",
    "\n",
    "Please review Knet function `progress!` and iterator function `ncycle` used below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNLM(train_vocab, HIST, EMBED, HIDDEN, DROPOUT)\n",
    "train_batches = collect(LMData(train_sentences))\n",
    "valid_batches = collect(LMData(valid_sentences))\n",
    "test_batches = collect(LMData(test_sentences))\n",
    "train_batches50 = train_batches[1:50] # Small sample for quick loss calculation\n",
    "\n",
    "epoch = adam(loss_v3, ((model, batch) for batch in train_batches))\n",
    "bestmodel, bestloss = deepcopy(model), maploss(loss_v3, model, valid_batches)\n",
    "\n",
    "progress!(ncycle(epoch, 100), seconds=5) do x\n",
    "    global bestmodel, bestloss\n",
    "    # Report gradient norm for the first batch\n",
    "    f = @diff loss_v3(model, train_batches[1])\n",
    "    gnorm = sqrt(sum(norm(grad(f,x))^2 for x in params(model)))\n",
    "    # Report training and validation loss\n",
    "    trnloss = maploss(loss_v3, model, train_batches50)\n",
    "    devloss = maploss(loss_v3, model, valid_batches)\n",
    "    # Save model that does best on validation data\n",
    "    if devloss < bestloss\n",
    "        bestmodel, bestloss = deepcopy(model), devloss\n",
    "    end\n",
    "    (trn=exp(trnloss), dev=exp(devloss), ∇=gnorm)\n",
    "end\n",
    "\n",
    "@info \"Validation perplexity of the best model: $(exp(bestloss))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can generate some original sentences with your trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# julia> generate(bestmodel)\n",
    "# \"the nasdaq composite index finished at N compared with ual earlier in the statement\"\n",
    "#\n",
    "# julia> generate(bestmodel)\n",
    "# \"in the pentagon joseph r. waertsilae transactions the 1\\\\/2-year transaction was oversubscribed an analyst at <unk>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "*This notebook was generated using [Literate.jl](https://github.com/fredrikekre/Literate.jl).*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.5.3",
   "language": "julia",
   "name": "julia-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 3
}
