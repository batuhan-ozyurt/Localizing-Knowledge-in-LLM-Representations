{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aa23e3-047d-4ad4-8a99-69e12d1aa6d5",
   "metadata": {},
   "source": [
    "## Necessary Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb66d3f0-d13b-4143-9d58-e4ccd61c1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "import xmltodict\n",
    "import pickle\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "import Levenshtein as lev\n",
    "from rouge import Rouge\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e0e4757b-6ec9-4ef3-95e6-3dc6d230b8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51acae2e-a5d1-4376-a9a7-f13f8619b170",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e4f776-fa5e-45ce-8865-091592aa4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_andersen = \"/kuacc/users/bozyurt20/ChildrenStories/Andersen\"\n",
    "path_fanny = \"/kuacc/users/bozyurt20/ChildrenStories/Fanny Fern\"\n",
    "path_annotations = \"/kuacc/users/bozyurt20/ChildrenStories/Annotations\"\n",
    "\n",
    "dir_list_andersen = os.listdir(path_andersen)\n",
    "dir_list_fanny = os.listdir(path_fanny)\n",
    "dir_list_annotations = os.listdir(path_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1e78476-5a38-4af9-9b30-85a66dba3e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.9/site-packages/transformers/models/t5/tokenization_t5.py:164: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6d4df58-e853-454c-8853-e415099f1718",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce4d168b-1311-413d-a6b1-1e02bba4d976",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b70e5b-c8d3-40e1-a4f2-07b1203a37a3",
   "metadata": {},
   "source": [
    "## Example Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "aeed57cf-752a-4bcd-8dd1-ca705e7e21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer.encode( \"Translate fromEnglish to German: I hate that they cancelled my membership.\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "10177792-1d02-40dc-ba38-845a6aa1531a",
   "metadata": {},
   "outputs": [],
   "source": [
    "               \n",
    "inputs = inputs.to(\"cuda:0\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(inputs)\n",
    "\n",
    "out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b87594cc-083d-4a7b-8e16-4cc3d3a9b2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ich hasse, dass sie meine Mitgliedschaft storniert.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8629dc-f5da-402e-b326-da4914bae2e6",
   "metadata": {},
   "source": [
    "## Prompt Creating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "67fff388-8f96-49af-804a-0f101bbe03a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompt_clipped(version, context, character, grammatical_number, max_no_tokens=512):\n",
    "    \n",
    "    if grammatical_number == 'singular':\n",
    "        to_be = 'is'\n",
    "    elif grammatical_number == 'plural':\n",
    "        to_be = 'are'\n",
    "    \n",
    "    if version in [1, 2, 9, 10, 11, 12, 13, 20, 21, 22]:\n",
    "        question = \"Where \" + to_be + \" \" + character + \"?\"\n",
    "    elif version in [4, 5, 7, 8, 15, 16, 18, 19]:\n",
    "        question = \"where \" + character + \" \" + to_be + \".\"\n",
    "    elif version in [3, 14]:\n",
    "        question = \"where \" + character + \" \" + to_be + \"?\"\n",
    "    elif version in [6, 17]:\n",
    "        question = \"where \" + to_be + \" \" + character + \"?\"\n",
    "        \n",
    "    if version == 1 or version == 12:\n",
    "        intro = \"Answer the question depending on the context.\"\n",
    "    elif version == 2 or version == 13:\n",
    "        intro = \"What is the answer?\"\n",
    "    elif version == 3 or version == 14:\n",
    "        intro = \"Can you tell me \"\n",
    "    elif version == 4 or version == 15:\n",
    "        intro = \"Please tell me \"\n",
    "    elif version == 5 or version == 16:\n",
    "        intro = \"Tell me \"\n",
    "    elif version == 6 or version == 17:\n",
    "        intro = \"From the passage, \"\n",
    "    elif version == 7 or version == 18:\n",
    "        intro = \"I want to know \"\n",
    "    elif version == 8 or version == 19:\n",
    "        intro = \"I want to ask \"\n",
    "    elif version == 9 or version == 20:\n",
    "        intro = \"What is the answer to: \"\n",
    "    elif version == 10 or version == 21:\n",
    "        intro = \"Find the answer to: \"\n",
    "    elif version == 11 or version == 22:\n",
    "        intro = \"Answer: \"     \n",
    "    \n",
    "    if version in [1, 2]:\n",
    "        oo = 0\n",
    "        tm = Template(\"\"\"{{ intro }}\n",
    "Context: {{context}};\n",
    "Question: {{question}};\n",
    "Answer: \"\"\")        \n",
    "        prompt = tm.render(intro=intro, context=context, question=question)\n",
    "        \n",
    "        while len(tokenizer.encode(prompt)) > max_no_tokens:\n",
    "            context = tokenizer.encode(context)\n",
    "            diff = len(tokenizer.encode(prompt)) - max_no_tokens\n",
    "            context = context[diff:]\n",
    "            oo += 1\n",
    "            if oo > 4:\n",
    "                context = context[1:]\n",
    "            context = tokenizer.decode(context, skip_special_tokens=True)\n",
    "            prompt = tm.render(intro=intro, context=context, question=question)\n",
    "        \n",
    "    elif version in [3, 4, 5, 6, 7, 8, 9, 10, 11]:\n",
    "        oo = 0\n",
    "        tm = Template(\"{{context}} {{intro}}{{question}}\")\n",
    "        prompt = tm.render(intro=intro, context=context, question=question)\n",
    "        while len(tokenizer.encode(prompt)) > max_no_tokens:\n",
    "            context = tokenizer.encode(context)\n",
    "            diff = len(tokenizer.encode(prompt)) - max_no_tokens\n",
    "            context = context[diff:]            \n",
    "            oo += 1\n",
    "            if oo > 4:\n",
    "                context = context[1:]\n",
    "            context = tokenizer.decode(context, skip_special_tokens=True)\n",
    "            prompt = tm.render(intro=intro, context=context, question=question)\n",
    "        \n",
    "        \n",
    "    elif version in [12, 13]:\n",
    "        oo = 0\n",
    "        tm = Template(\"\"\"{{ intro }}\n",
    "Context: {{context}};\n",
    "Question: {{question}};\n",
    "If you can't find the answer, please respond \"unanswerable\".\n",
    "Answer: \"\"\")\n",
    "        prompt = tm.render(intro=intro, context=context, question=question)\n",
    "        while len(tokenizer.encode(prompt)) > max_no_tokens:\n",
    "            context = tokenizer.encode(context)\n",
    "            diff = len(tokenizer.encode(prompt)) - max_no_tokens\n",
    "            context = context[diff:]\n",
    "            oo += 1\n",
    "            if oo > 4:\n",
    "                context = context[1:]\n",
    "            context = tokenizer.decode(context, skip_special_tokens=True)\n",
    "            prompt = tm.render(intro=intro, context=context, question=question)\n",
    "        \n",
    "    elif version in [14, 15, 16, 17, 18, 19, 20, 21, 22]:\n",
    "        oo = 0\n",
    "        tm = Template('{{context}} {{intro}}{{question}} If you can\\'t find the answer, please respond \"unanswerable\".\"')\n",
    "        prompt = tm.render(intro=intro, context=context, question=question)    \n",
    "        while len(tokenizer.encode(prompt)) > max_no_tokens:\n",
    "            context = tokenizer.encode(context)\n",
    "            diff = len(tokenizer.encode(prompt)) - max_no_tokens\n",
    "            context = context[diff:]\n",
    "            oo += 1\n",
    "            if oo > 4:\n",
    "                context = context[1:]\n",
    "            context = tokenizer.decode(context, skip_special_tokens=True)\n",
    "            prompt = tm.render(intro=intro, context=context, question=question)\n",
    "            \n",
    "    elif version == 23:\n",
    "        oo = 0\n",
    "        prompt = \"Where \" + to_be + \" \" + character + \" in the following text: \" + context + \" Answer: \"\n",
    "        while len(tokenizer.encode(prompt)) > max_no_tokens:\n",
    "            context = tokenizer.encode(context)\n",
    "            diff = len(tokenizer.encode(prompt)) - max_no_tokens\n",
    "            context = context[diff:]\n",
    "            oo += 1\n",
    "            if oo > 4:\n",
    "                context = context[1:]\n",
    "            context = tokenizer.decode(context, skip_special_tokens=True)\n",
    "            prompt = \"Where \" + to_be + \" \" + character + \" in the following text: \" + context + \" Answer: \"\n",
    "        \n",
    "    return prompt, context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd7db9b-080f-4d28-9411-933ea465493e",
   "metadata": {},
   "source": [
    "## Accuracy Calculating Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccfa7c8b-360c-48dd-b2bc-b6c6125ec2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def exact_match(predictions):\n",
    "\n",
    "    matches_exact = {}\n",
    "\n",
    "    for item in predictions:\n",
    "\n",
    "        matches_exact[item] = [ [] for _ in range(1,24)]\n",
    "\n",
    "        f = open(os.path.join(path_annotations, item), 'r')\n",
    "        annotations = pd.read_csv(f, sep=\"\\t\")\n",
    "        annotations = annotations.values #numpy array\n",
    "        f.close()\n",
    "\n",
    "        for k in range(1,24):\n",
    "\n",
    "            pred_locs = predictions[item][k-1]\n",
    "            i = 0\n",
    "\n",
    "            for line in annotations:\n",
    "\n",
    "                character = line[1]\n",
    "                gold_locations = line[2].split(\"/\")\n",
    "                \n",
    "                pred_tokenized = word_tokenize(pred_locs[i].lower())\n",
    "                new_pred_tokens = [ token for token in pred_tokenized if token not in stop_words]\n",
    "                pred_wo_stop_words = \" \".join(new_pred_tokens)\n",
    "                \n",
    "                char_tokenized = word_tokenize(character.lower())\n",
    "                new_char_tokens = [ token for token in char_tokenized if token not in stop_words]\n",
    "                char_wo_stop_words = \" \".join(new_char_tokens)\n",
    "                \n",
    "                if char_wo_stop_words not in \" \".join(gold_locations):\n",
    "                    pred_wo_stop_words = pred_wo_stop_words.replace(char_wo_stop_words, \"\")   \n",
    "                \n",
    "                else:\n",
    "                    if pred_wo_stop_words[len(char_wo_stop_words)+1:len(char_wo_stop_words)+3] == \"is\" or pred_wo_stop_words[len(char_wo_stop_words)+1:len(char_wo_stop_words)+4] == \"are\":\n",
    "                        pred_wo_stop_words = pred_wo_stop_words[len(char_wo_stop_words)+1:]\n",
    "\n",
    "                match = False\n",
    "\n",
    "                for gold_location in gold_locations:\n",
    "\n",
    "                    gold_tokenized = word_tokenize(gold_location.lower())\n",
    "                    new_gold_tokens = [ token for token in gold_tokenized if token not in stop_words]\n",
    "                    gold_wo_stop_words = \" \".join(new_gold_tokens)\n",
    "\n",
    "                    if gold_wo_stop_words == pred_wo_stop_words:\n",
    "                        match = True\n",
    "\n",
    "                if match:\n",
    "                    matches_exact[item][k-1].append(1)\n",
    "                else:\n",
    "                    matches_exact[item][k-1].append(0)\n",
    "\n",
    "                i += 1\n",
    "    \n",
    "    return matches_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5610e7e6-ba92-4ea1-8e5e-25e6c32e4b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(predictions):\n",
    "    \n",
    "    matches_fuzzy = {}\n",
    "\n",
    "    for item in predictions:\n",
    "\n",
    "        print(item)\n",
    "        matches_fuzzy[item] = [ [] for _ in range(1,24)]\n",
    "\n",
    "        f = open(os.path.join(path_annotations, item), 'r')\n",
    "        annotations = pd.read_csv(f, sep=\"\\t\")\n",
    "        annotations = annotations.values #numpy array\n",
    "        f.close()\n",
    "\n",
    "        for k in range(1,24):\n",
    "\n",
    "            pred_locs = predictions[item][k-1]        \n",
    "            i = 0\n",
    "\n",
    "            for line in annotations:\n",
    "\n",
    "                gold_locations = line[2].split(\"/\")\n",
    "\n",
    "                pred_tokenized = word_tokenize(pred_locs[i].lower())\n",
    "                new_pred_tokens = [ token for token in pred_tokenized if token not in stop_words ]\n",
    "                pred_wo_stop_words = \" \".join(new_pred_tokens)\n",
    "                \n",
    "                char_tokenized = word_tokenize(character.lower())\n",
    "                new_char_tokens = [ token for token in char_tokenized if token not in stop_words]\n",
    "                char_wo_stop_words = \" \".join(new_char_tokens)\n",
    "                \n",
    "                if char_wo_stop_words not in \" \".join(gold_locations):\n",
    "                    pred_wo_stop_words = pred_wo_stop_words.replace(char_wo_stop_words, \"\")   \n",
    "                \n",
    "                else:\n",
    "                    if pred_wo_stop_words[len(char_wo_stop_words)+1:len(char_wo_stop_words)+3] == \"is\" or pred_wo_stop_words[len(char_wo_stop_words)+1:len(char_wo_stop_words)+4] == \"are\":\n",
    "                        pred_wo_stop_words = pred_wo_stop_words[len(char_wo_stop_words)+1:]\n",
    "\n",
    "                match = False\n",
    "\n",
    "                for gold_location in gold_locations:\n",
    "\n",
    "                    gold_tokenized = word_tokenize(gold_location.lower())\n",
    "                    new_gold_tokens = [ token for token in gold_tokenized if token not in stop_words ]\n",
    "                    gold_wo_stop_words = \" \".join(new_gold_tokens)\n",
    "\n",
    "                    if fuzz.partial_ratio(gold_wo_stop_words, pred_wo_stop_words) > 90:\n",
    "                        match = True\n",
    "\n",
    "                if match: \n",
    "                    matches_fuzzy[item][k-1].append(1)\n",
    "                else:\n",
    "                    matches_fuzzy[item][k-1].append(0)\n",
    "\n",
    "                i += 1\n",
    "                \n",
    "    return matches_fuzzy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d3549b-656d-46b3-bb8a-82e885e30450",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2b5058ff-7cb3-4693-9f30-ea199843af4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_clean_ending(example_text):\n",
    "    example_text = example_text.rstrip(\", ;-\\n\")\n",
    "    if example_text[-1] != \".\":\n",
    "        example_text += \".\"\n",
    "    return example_text\n",
    "\n",
    "def remove_new_lines(text):\n",
    "    paragraphs = text.split(\"\\n\\n\")\n",
    "    new_paragraphs = []\n",
    "    for paragraph in paragraphs:\n",
    "        new_paragraphs.append(paragraph.replace(\"\\n\", \" \"))\n",
    "    new_text = \"\\n\".join(new_paragraphs)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "306d739c-fc82-4290-8d45-86c243147148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story2.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (708 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story8.txt\n",
      "Andersen_story11.txt\n",
      "Andersen_story7.txt\n",
      "Andersen_story17.txt\n",
      "Andersen_story15.txt\n",
      "Andersen_story9.txt\n",
      "Andersen_story5.txt\n",
      "Andersen_story1.txt\n",
      "Andersen_story12.txt\n",
      "Andersen_story16.txt\n",
      "Andersen_story18.txt\n",
      "Andersen_story3.txt\n",
      "Andersen_story10.txt\n",
      "Andersen_story13.txt\n"
     ]
    }
   ],
   "source": [
    "# prompt has the max number of tokens: 512, and we start at a \" \" char.\n",
    "\n",
    "m4_predictions = {}\n",
    "\n",
    "for item in dir_list_andersen:\n",
    "    \n",
    "    if item in dir_list_annotations:\n",
    "        \n",
    "        print(item)\n",
    "        \n",
    "        f = open(os.path.join(path_andersen, item), 'r') \n",
    "        story = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        out_path = \"T5_Method4_\" + item[:-3] + \"xlsx\"\n",
    "        writer = pd.ExcelWriter(out_path, engine='xlsxwriter')\n",
    "        workbook = writer.book\n",
    "        format = workbook.add_format({'text_wrap': True})\n",
    "        \n",
    "        m4_predictions[item] = [ [] for _ in range(1,24)]\n",
    "        \n",
    "        f = open(os.path.join(path_annotations, item), 'r')\n",
    "        annotations = pd.read_csv(f, sep=\"\\t\")\n",
    "        annotations = annotations.values\n",
    "        f.close()\n",
    "        \n",
    "        i = 0\n",
    "        \n",
    "        paragraphs = story.split(\"\\n\\n\")\n",
    "        paragraph = paragraphs[0]\n",
    "        len_title = len(paragraph) + 2        \n",
    "    \n",
    "        for line in annotations:\n",
    "            \n",
    "            character = line[1]\n",
    "            gold_answer = line[2]\n",
    "            grammatical_number = line[3]\n",
    "\n",
    "            gold_locations = gold_answer.split(\"/\")\n",
    "            my_dic = {\"Prompts\": [gold_answer, \"-\", \"-\"]}\n",
    "            \n",
    "            for k in range(1, 24):\n",
    "                \n",
    "                y = line[0]\n",
    "                x = y - 5120\n",
    "\n",
    "                if x < len_title:\n",
    "                    text = story[len_title:y]\n",
    "\n",
    "                else:\n",
    "                    x = story[x:y].find(\" \") + x\n",
    "                    text = story[x:y]                \n",
    "                \n",
    "                text = text_clean_ending(text)\n",
    "                text = remove_new_lines(text)                    \n",
    "                \n",
    "                prompt, context2 = create_prompt_clipped(k, text, character, grammatical_number, 512)\n",
    "                inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "                \n",
    "                inputs = inputs.to(\"cuda:0\")\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    outputs = model.generate(inputs)\n",
    "                    \n",
    "                out = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "                \n",
    "                match1 = \"No\"\n",
    "                match2 = \"No\"\n",
    "                \n",
    "                pred_tokenized = word_tokenize(out.lower())\n",
    "                new_pred_tokens = [ token for token in pred_tokenized if token not in stop_words ]\n",
    "                pred_wo_stop_words = \" \".join(new_pred_tokens) \n",
    "                \n",
    "                for gold_location in gold_locations:\n",
    "                    \n",
    "                    gold_tokenized = word_tokenize(gold_location.lower())\n",
    "                    new_gold_tokens = [ token for token in gold_tokenized if token not in stop_words ]\n",
    "                    gold_wo_stop_words = \" \".join(new_gold_tokens)\n",
    "                    \n",
    "                    if gold_wo_stop_words == pred_wo_stop_words:\n",
    "                        match1 = \"Yes\"\n",
    "                        \n",
    "                    if fuzz.partial_ratio(gold_wo_stop_words, pred_wo_stop_words) > 90:\n",
    "                        match2 = \"Yes\"\n",
    "                        \n",
    "                my_dic[prompt] = [out, match1, match2]\n",
    "                m4_predictions[item][k-1].append(out)  \n",
    "                \n",
    "            df = pd.DataFrame(data=my_dic, index=[\"output\", \"exact match?\", \"fuzzy match?\"])\n",
    "            df = (df.T)\n",
    "            df.to_excel(writer, sheet_name=str(i+1))\n",
    "            worksheet = writer.sheets[str(i+1)]\n",
    "            \n",
    "            i += 1\n",
    "            \n",
    "            for idx, col in enumerate(df):\n",
    "                max_len = 75\n",
    "                worksheet.set_column(idx, idx, max_len, format)\n",
    "        \n",
    "        writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb5bb8c8-6cfd-4248-ae4f-0c281481ad77",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"T5_predictions_1.txt\", \"wb\") as f:\n",
    "    pickle.dump(m4_predictions, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d133df-47b5-4cd6-9f88-131141bd5b62",
   "metadata": {},
   "source": [
    "## Calculating the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d8012-3ffc-49cf-8333-a06a9ac1e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Method4Predictions_distraction.txt\", \"rb\") as f:\n",
    "    m4_predictions = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2b6fa396-1537-4280-8484-ecd1f83a96c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_matches_exact = exact_match(m4_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33778cab-73fb-4434-a76e-0f90c855e0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_accuracy_exact = {}\n",
    "\n",
    "for item in m4_matches_exact:\n",
    "    m4_accuracy_exact[item] = []\n",
    "    for prompt_version in m4_matches_exact[item]:\n",
    "        m4_accuracy_exact[item].append(np.mean(np.array(prompt_version)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "00acf7da-dd32-4b5b-b4ad-cd469c59249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_prompt_accuracies_exact = [ [] for _ in range(23)]\n",
    "\n",
    "for k in range(23):\n",
    "    for item in m4_accuracy_exact:\n",
    "        m4_prompt_accuracies_exact[k].append(m4_accuracy_exact[item][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4d32a36-b37f-42b7-8ae8-74e8fd302f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_prompt_accuracy_exact = np.mean(np.array(m4_prompt_accuracies_exact), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "56d387f8-3750-478d-822b-ae42af3b06fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_exact.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "29a419a2-7540-4123-93a8-70595cb57b42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_exact[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "91d48588-bb09-445c-a93f-c103ce1af284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_exact.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "850b1f19-17e2-4b87-af79-f574aa282662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc7ef46f-e317-440b-a591-8231ae03cf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story2.txt\n",
      "Andersen_story8.txt\n",
      "Andersen_story11.txt\n",
      "Andersen_story7.txt\n",
      "Andersen_story17.txt\n",
      "Andersen_story15.txt\n",
      "Andersen_story9.txt\n",
      "Andersen_story5.txt\n",
      "Andersen_story1.txt\n",
      "Andersen_story12.txt\n",
      "Andersen_story16.txt\n",
      "Andersen_story18.txt\n",
      "Andersen_story3.txt\n",
      "Andersen_story10.txt\n",
      "Andersen_story13.txt\n"
     ]
    }
   ],
   "source": [
    "m4_matches_fuzzy = fuzzy_match(m4_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "de910f07-cf3f-406f-b657-ccab936ea442",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_accuracy_fuzzy = {}\n",
    "\n",
    "for item in m4_matches_fuzzy:\n",
    "    m4_accuracy_fuzzy[item] = []\n",
    "    for prompt_version in m4_matches_fuzzy[item]:\n",
    "        m4_accuracy_fuzzy[item].append(np.mean(np.array(prompt_version)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04c4f715-2037-41e1-9f6c-a9ef10f01acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_prompt_accuracies_fuzzy = [ [] for _ in range(23)]\n",
    "\n",
    "for k in range(23):\n",
    "    for item in m4_accuracy_fuzzy:\n",
    "        m4_prompt_accuracies_fuzzy[k].append(m4_accuracy_fuzzy[item][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ae69b52-b126-4889-b84d-fcc2b0c78101",
   "metadata": {},
   "outputs": [],
   "source": [
    "m4_prompt_accuracy_fuzzy = np.mean(np.array(m4_prompt_accuracies_fuzzy), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1f67d1b9-5c92-4004-bc51-fbf171d51335",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_fuzzy.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8ec31fb-67aa-4efa-8f87-9111fcb118b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11850652036135906"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_fuzzy[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9f165a0-d849-4779-9781-352bcec533f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.10096236434595482"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m4_prompt_accuracy_fuzzy.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c733d6b8-704d-4302-b1b6-8c557a2c2cad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
