{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aa23e3-047d-4ad4-8a99-69e12d1aa6d5",
   "metadata": {},
   "source": [
    "## Necessary Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb66d3f0-d13b-4143-9d58-e4ccd61c1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add here your imports\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "import xmltodict\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58afed2-0667-46ba-b632-f141b91a7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/utilities')\n",
    "from util_research import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f3428f5-8a18-4221-bfea-fd3ac50bc189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "426564b991b0486db0599f0bae9d57d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  'mosaicml/mpt-7b-storywriter',\n",
    "  trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b36dcefc-d6cf-436a-8601-95f8d808035d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPTForCausalLM(\n",
       "  (transformer): MPTModel(\n",
       "    (wte): Embedding(50432, 4096)\n",
       "    (emb_drop): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (8): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (9): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (10): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (11): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (12): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (13): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (14): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (15): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (16): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (17): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (18): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (19): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (20): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (21): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (22): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (23): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (24): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (25): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (26): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (27): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (28): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (29): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (30): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (31): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e7623fd-0efb-4838-b211-bf3c689ba111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._modules[\"transformer\"]._modules[\"norm_f\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "05a7169d-9109-42b4-a8ba-cdfed4466de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7fd3434807c4f9eb5c613a87836a8e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/156 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e08a964f325349a9940f097b12b58ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/1.08M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8edb46ca25c64130b2d1dc6421c667bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/457k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37d6f269cf2f44548ee57727e7e079dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d36a29e024c4b83af9440fca5fc4486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/90.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac4079e4-3807-47d6-8bef-f3a226891ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write me a 100,000 word story about a princess.\"\n",
    "input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06028444-0985-4ec2-af94-35dca76b9f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/kuacc/users/bozyurt20/.conda/envs/hf/lib/python3.8/site-packages/transformers/generation/utils.py:1315: UserWarning: Using `max_length`'s default (20) to control the generation length. This behaviour is deprecated and will be removed from the config in v5 of Transformers -- we recommend using `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[10639,   479,   247,  2233,    13,   933,  3159,  2926,   670,   247,\n",
       "         28806,    15,   187,   187,    42,   369,   275,   253,  4766,   273]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77e77cdc-0636-4ba5-8a29-85bd2cd28fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torch.Tensor([[10639,   479,   247,  2233,    13,   933,  3159,  2926,   670,   247,\n",
    "         28806,    15,   187,   187,    42,   369,   275,   253,  4766,   273]]).int()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a564b13-51f5-4036-a1f9-5262552bc95b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Write me a 100,000 word story about a princess.\\n\\nI was in the middle of'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "868e054e-750c-4372-97d2-8ad2e6abb9f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explicitly passing a `revision` is encouraged when loading a configuration with custom code to ensure no malicious code has been contributed in a newer revision.\n",
      "Explicitly passing a `revision` is encouraged when loading a model with custom code to ensure no malicious code has been contributed in a newer revision.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "467335f418994c32a6666f8733af5fa3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "model2 = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  'mosaicml/mpt-7b-instruct',\n",
    "  trust_remote_code=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e4f776-fa5e-45ce-8865-091592aa4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add the number of template options for prompting\n",
    "\n",
    "num_templates = 23\n",
    "\n",
    "path_andersen = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Andersen\"\n",
    "path_fanny = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Fanny Fern\"\n",
    "path_annotations = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Annotations\"\n",
    "\n",
    "dir_list_andersen = os.listdir(path_andersen)\n",
    "dir_list_fanny = os.listdir(path_fanny)\n",
    "dir_list_annotations = os.listdir(path_annotations)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1a391456-3ee7-4fbf-bba8-b035fa6b9cfb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoModelforCondititonalGeneration' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m## TODO: Add Your model and tokenizer\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mAutoModelforCondititonalGeneration\u001b[49m\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggyllama/llama-7b\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggyllama/llama-7be\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoModelforCondititonalGeneration' is not defined"
     ]
    }
   ],
   "source": [
    "## TODO: Add Your model and tokenizer\n",
    "\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "  'mosaicml/mpt-7b-instruct',\n",
    "  trust_remote_code=True\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_instance = ModelInstance(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b06b3c9-ca22-4560-9d30-45719c624392",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-neox-20b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c49bf8a8-4e6d-487e-a538-e072db4b47f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.logit_scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d27bc-1629-4662-92c7-7d6b25ee04f3",
   "metadata": {},
   "source": [
    "## Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e7eac3-656c-42eb-a6df-96b5141694dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 32\n",
    "d_model = 4096\n",
    "max_len = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9485670-ffc1-4b0f-8b5e-ad6ec0700489",
   "metadata": {},
   "outputs": [],
   "source": [
    "cp modeling_mpt.py /kuacc/users/bozyurt20/.cache/huggingface/hub/models--mosaicml--mpt-7b-instruct/blobs/3bd262185584184ae27a6eac6173c1c3c62feb7e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "737bf0d5-732d-4be5-8466-b41a9079cb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_first_timestep(first_prompt, entity_names):\n",
    "    input_ids = tokenizer.encode(first_prompt, truncation=True, max_length=max_len, return_tensors=\"pt\")\n",
    "    len_input_ids = len(input_ids[0])\n",
    "    dec_out = model(input_ids, output_hidden_states=True) \n",
    "    dec_hidden_states = dec_out.hidden_states # 32 x (1, T, d)\n",
    "    dec_final_hidden = dec_out.\n",
    "    dec_hidden = torch.zeros(num_enc_layers, len_input_ids, d_hidden) # (24, T, d)\n",
    "\n",
    "    for i, hidden in enumerate(enc_out[1]):\n",
    "        if i != 0:\n",
    "            enc_hidden[i-1:i, :, :] = hidden\n",
    "\n",
    "    entity_name = entity_names[0]\n",
    "    entity_ind = find_index(input_ids, entity_name)\n",
    "    entities_hidden_states = enc_hidden[:, entity_ind, :].unsqueeze(0)\n",
    "    \n",
    "    for entity_name in entity_names[1:]:\n",
    "        entity_ind = find_index(input_ids, entity_name)\n",
    "        entity_hidden_states = enc_hidden[:, entity_ind, :].unsqueeze(0)\n",
    "        entities_hidden_states = torch.cat((entities_hidden_states,\n",
    "                                       entity_hidden_states), dim=0)\n",
    "    print(entities_hidden_states.shape)\n",
    "    return entities_hidden_states\n",
    "\n",
    "def find_index(input_ids, entity_str):\n",
    "    \n",
    "    entity_id = tokenizer.encode(entity_str)\n",
    "    if len(entity_id) != 2:\n",
    "        print(\"Not an appropriate entity!\")\n",
    "        return\n",
    "    entity_id = entity_id[0]\n",
    "    \n",
    "    input_ids_list = input_ids.tolist()\n",
    "\n",
    "    all_entity_mention_indices = []\n",
    "    for i, j in enumerate(input_ids_list[0]):\n",
    "        if j == entity_id:\n",
    "            all_entity_mention_indices.append(i)\n",
    "    try:\n",
    "        entity_ind = all_entity_mention_indices[-1]\n",
    "        return entity_ind\n",
    "    except:\n",
    "        print(\"entity not found in the input!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b70e5b-c8d3-40e1-a4f2-07b1203a37a3",
   "metadata": {},
   "source": [
    "## Example Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "04c86fd8-46c1-4550-bce7-139ddf208f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Lilly was at the park. Jason was in London. Where was Lilly?\\nLilly was at the park. Jason was in London.\\nWhere was Lilly?\\nLilly was at the']\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Lilly was at the park. Jason was in London. Where was Lilly?\"\n",
    "\n",
    "input_ids = tokenizer.encode(prompt, max_length=2048, return_tensors=\"pt\")\n",
    "\n",
    "summary_ids = model.generate(input_ids, max_length=40)\n",
    "out = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cded5e33-0952-4a1d-8826-1eca63a64e60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9b9adc7f-7b4d-4ff6-826b-eff956c7bcf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids) # only logits are outputted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a596c69f-4c6b-494a-9d93-b29bf8cc75e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a0257ebf-ed23-4189-a6ed-30fca52ddb97",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "61287c27-2f3a-4095-8ec5-f303258252a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 16, 50432])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "5003cef6-9f25-4e62-9dea-9cb4c82bea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(input_ids, output_hidden_states=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "876fd97b-95e5-4939-95bc-7aaa81907f23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'logits', 'past_key_values', 'hidden_states', 'attentions'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "fc929528-89fa-40dd-a660-ab675151122f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(outputs.hidden_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7f49e0ef-669c-4f70-ba16-3d7cea2fed66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n",
      "torch.Size([1, 16, 4096])\n"
     ]
    }
   ],
   "source": [
    "for hidden in outputs.hidden_states:\n",
    "    print(hidden.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "1945a909-e050-4457-93ad-be62a50cee88",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method GenerationMixin.generate of MPTForCausalLM(\n",
       "  (transformer): MPTModel(\n",
       "    (wte): Embedding(50432, 4096)\n",
       "    (emb_drop): Dropout(p=0, inplace=False)\n",
       "    (blocks): ModuleList(\n",
       "      (0): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (1): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (2): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (3): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (4): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (5): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (6): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (7): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (8): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (9): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (10): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (11): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (12): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (13): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (14): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (15): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (16): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (17): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (18): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (19): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (20): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (21): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (22): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (23): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (24): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (25): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (26): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (27): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (28): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (29): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (30): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "      (31): MPTBlock(\n",
       "        (norm_1): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): MultiheadAttention(\n",
       "          (Wqkv): Linear(in_features=4096, out_features=12288, bias=False)\n",
       "          (out_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (norm_2): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "        (ffn): MPTMLP(\n",
       "          (up_proj): Linear(in_features=4096, out_features=16384, bias=False)\n",
       "          (act): GELU(approximate='none')\n",
       "          (down_proj): Linear(in_features=16384, out_features=4096, bias=False)\n",
       "        )\n",
       "        (resid_attn_dropout): Dropout(p=0, inplace=False)\n",
       "        (resid_ffn_dropout): Dropout(p=0, inplace=False)\n",
       "      )\n",
       "    )\n",
       "    (norm_f): LPLayerNorm((4096,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "f303fe0b-ac84-4b65-8352-0a3d6f3fb586",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 4)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ()\n",
    "x += (5,)\n",
    "x += (4,)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8494bbe-7f28-4f41-b853-34e9a7d3b8ec",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f750be48-4a4c-43e9-85d0-46068af0d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model_instance):\n",
    "    \n",
    "    model = model_instance.model\n",
    "    tokenizer = model_instance.tokenizer\n",
    "    device = model_instance.device\n",
    "    \n",
    "    all_result_objects = defaultdict(list)\n",
    "    \n",
    "    for item in dir_list_annotations:\n",
    "\n",
    "        print(item)\n",
    "\n",
    "        story_no = item[len(\"Andersen_story\"):-len(\".txt\")]\n",
    "\n",
    "        f = open(os.path.join(path_andersen, item), 'r') \n",
    "        story = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        paragraphs = story.split(\"\\n\\n\")\n",
    "        paragraph = paragraphs[0]\n",
    "        len_title = len(paragraph) + 2   \n",
    "        \n",
    "        annotations = all_annotations[item]\n",
    "        \n",
    "        for line in annotations:\n",
    "            \n",
    "            ind = line[0]\n",
    "            character = line[1]\n",
    "            gold_answer = line[2]\n",
    "            grammatical_number = line[3]\n",
    "\n",
    "            gold_locations = gold_answer.split(\"/\")\n",
    "            \n",
    "            for k in range(1, num_templates+1):\n",
    "                \n",
    "                y = line[0]\n",
    "                x = y - 5120\n",
    "\n",
    "                if x < len_title:\n",
    "                    text = story[len_title:y]\n",
    "\n",
    "                else:\n",
    "                    x = story[x:y].find(\" \") + x\n",
    "                    text = story[x:y]                \n",
    "                \n",
    "                text = text_clean_ending(text)\n",
    "                text = remove_new_lines(text)                    \n",
    "                \n",
    "                ## TODO: Write the predictor lines\n",
    "                \n",
    "                prompt, context2 = create_prompt_clipped(tokenizer, k, text, character, grammatical_number, 1024)\n",
    "                \n",
    "                inputs = tokenizer([prompt], max_length=1024, return_tensors=\"pt\")\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    summary_ids = model.generate(input_ids, min_length=0, max_length=20)\n",
    "                    \n",
    "                out = tokenizer.decode(summary_ids[0], skip_special_tokens=True)                \n",
    "                \n",
    "                ## Ends here\n",
    "                \n",
    "                match1, match2 = exactly_or_fuzzily_matched(out, character, gold_locations)\n",
    "                result_object = ResultObject(prompt, out, ind, character, gold_locations, story_no, k, match1, match2)\n",
    "                all_result_objects[k].append(result_object)\n",
    "                \n",
    "    return all_result_objects\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da26510-473e-4a18-8ca3-71313c735f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story11.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story12.txt\n",
      "Andersen_story13.txt\n",
      "Andersen_story15.txt\n",
      "Andersen_story16.txt\n",
      "Andersen_story17.txt\n",
      "Andersen_story18.txt\n",
      "Andersen_story1.txt\n",
      "Andersen_story2.txt\n",
      "Andersen_story3.txt\n",
      "Andersen_story5.txt\n",
      "Andersen_story7.txt\n",
      "Andersen_story8.txt\n",
      "Andersen_story9.txt\n",
      "Andersen_story10.txt\n"
     ]
    }
   ],
   "source": [
    "all_result_objects = predictor(model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49de2599-d756-4d7a-b50f-c8c89b2030c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: define a path name for saving the results\n",
    "\n",
    "with open(\"bart_large_predictions.txt\", \"wb\") as f:\n",
    "    pickle.dump(all_result_objects, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d133df-47b5-4cd6-9f88-131141bd5b62",
   "metadata": {},
   "source": [
    "## Calculating the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d8012-3ffc-49cf-8333-a06a9ac1e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write the path name that has the results\n",
    "\n",
    "with open(\"bart_large_predictions.txt\", \"rb\") as f:\n",
    "    all_result_objects = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02139fb-1958-4bb7-9a36-a90267a5e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculator(all_result_objects):\n",
    "    \n",
    "    exact_accuracies = defaultdict(list)\n",
    "    fuzzy_accuracies = defaultdict(list)\n",
    "    exact_averages = defaultdict(list)\n",
    "    fuzzy_averages = defaultdict(list)\n",
    "\n",
    "    for k in all_result_objects:\n",
    "        for result_object in all_result_objects[k]:\n",
    "            \"\"\"prompt = result_object.prompt\n",
    "            out = result_object.out\n",
    "            story_no = result_object.story_no\n",
    "            k = result_object.k\"\"\"\n",
    "            match1 = result_object.match1\n",
    "            match2 = result_object.match2\n",
    "            if match1 == \"Yes\":\n",
    "                exact_accuracies[k].append(1)\n",
    "            else:\n",
    "                exact_accuracies[k].append(0)\n",
    "            if match2 == \"Yes\":\n",
    "                fuzzy_accuracies[k].append(1)\n",
    "            else:\n",
    "                fuzzy_accuracies[k].append(0)\n",
    "            \n",
    "        exact_averages[k] = np.mean(np.array(exact_accuracies[k]))\n",
    "        fuzzy_averages[k] = np.mean(np.array(fuzzy_accuracies[k]))\n",
    "        \n",
    "    best_exact_k = np.array(list(exact_averages.values())).argmax() + 1\n",
    "    best_fuzzy_k = np.array(list(exact_averages.values())).argmax() + 1\n",
    "    \n",
    "    return exact_averages, fuzzy_averages, best_exact_k, best_fuzzy_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa2a240-ce18-44dd-8a3c-a0ddd029b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_averages, fuzzy_averages, best_exact_k, best_fuzzy_k = accuracy_calculator(all_result_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "419db42a-7ced-41fd-b7a9-43033f8936cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: 0.03614457831325301,\n",
       "             2: 0.0321285140562249,\n",
       "             3: 0.04819277108433735,\n",
       "             4: 0.05622489959839357,\n",
       "             5: 0.04819277108433735,\n",
       "             6: 0.04819277108433735,\n",
       "             7: 0.04819277108433735,\n",
       "             8: 0.04819277108433735,\n",
       "             9: 0.05220883534136546,\n",
       "             10: 0.04417670682730924,\n",
       "             11: 0.05220883534136546,\n",
       "             12: 0.03614457831325301,\n",
       "             13: 0.028112449799196786,\n",
       "             14: 0.06827309236947791,\n",
       "             15: 0.04819277108433735,\n",
       "             16: 0.05220883534136546,\n",
       "             17: 0.060240963855421686,\n",
       "             18: 0.060240963855421686,\n",
       "             19: 0.060240963855421686,\n",
       "             20: 0.04417670682730924,\n",
       "             21: 0.060240963855421686,\n",
       "             22: 0.05622489959839357,\n",
       "             23: 0.024096385542168676})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75209a2a-2a64-46ea-bf82-f8c3cf82b92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
