{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27aa23e3-047d-4ad4-8a99-69e12d1aa6d5",
   "metadata": {},
   "source": [
    "## Necessary Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb66d3f0-d13b-4143-9d58-e4ccd61c1d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: Add here your imports\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "import xmltodict\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e58afed2-0667-46ba-b632-f141b91a7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/')\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/text_injection')\n",
    "from text_injection.util_research import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57e4f776-fa5e-45ce-8865-091592aa4bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add the number of template options for prompting\n",
    "\n",
    "num_templates = 23\n",
    "\n",
    "path_andersen = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Andersen\"\n",
    "path_fanny = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Fanny Fern\"\n",
    "path_annotations = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Annotations\"\n",
    "\n",
    "dir_list_andersen = os.listdir(path_andersen)\n",
    "dir_list_fanny = os.listdir(path_fanny)\n",
    "dir_list_annotations = os.listdir(path_annotations)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a391456-3ee7-4fbf-bba8-b035fa6b9cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: Add Your model and tokenizer\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_instance = ModelInstance(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b70e5b-c8d3-40e1-a4f2-07b1203a37a3",
   "metadata": {},
   "source": [
    "## Example Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04c86fd8-46c1-4550-bce7-139ddf208f18",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PG&E stated it scheduled the blackouts in response to forecasts for high winds amid'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Â TODO: example pipeline\n",
    "\n",
    "from transformers import AutoTokenizer, BartForConditionalGeneration\n",
    "\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "\n",
    "ARTICLE_TO_SUMMARIZE = (\n",
    "    \"PG&E stated it scheduled the blackouts in response to forecasts for high winds \"\n",
    "    \"amid dry conditions. The aim is to reduce the risk of wildfires. Nearly 800 thousand customers were \"\n",
    "    \"scheduled to be affected by the shutoffs which were expected to last through at least midday tomorrow.\"\n",
    ")\n",
    "inputs = tokenizer([ARTICLE_TO_SUMMARIZE], max_length=1024, return_tensors=\"pt\")\n",
    "\n",
    "# Generate Summary\n",
    "summary_ids = model.generate(inputs[\"input_ids\"], num_beams=2, min_length=0, max_length=20)\n",
    "tokenizer.batch_decode(summary_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8494bbe-7f28-4f41-b853-34e9a7d3b8ec",
   "metadata": {},
   "source": [
    "## Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f750be48-4a4c-43e9-85d0-46068af0d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(model_instance):\n",
    "    \n",
    "    model = model_instance.model\n",
    "    tokenizer = model_instance.tokenizer\n",
    "    device = model_instance.device\n",
    "    \n",
    "    all_result_objects = defaultdict(list)\n",
    "    \n",
    "    for item in dir_list_annotations:\n",
    "\n",
    "        print(item)\n",
    "\n",
    "        story_no = item[len(\"Andersen_story\"):-len(\".txt\")]\n",
    "\n",
    "        f = open(os.path.join(path_andersen, item), 'r') \n",
    "        story = f.read()\n",
    "        f.close()\n",
    "        \n",
    "        paragraphs = story.split(\"\\n\\n\")\n",
    "        paragraph = paragraphs[0]\n",
    "        len_title = len(paragraph) + 2   \n",
    "        \n",
    "        annotations = all_annotations[item]\n",
    "        \n",
    "        for line in annotations:\n",
    "            \n",
    "            ind = line[0]\n",
    "            character = line[1]\n",
    "            gold_answer = line[2]\n",
    "            grammatical_number = line[3]\n",
    "\n",
    "            gold_locations = gold_answer.split(\"/\")\n",
    "            \n",
    "            for k in range(1, num_templates+1):\n",
    "                \n",
    "                y = line[0]\n",
    "                x = y - 5120\n",
    "\n",
    "                if x < len_title:\n",
    "                    text = story[len_title:y]\n",
    "\n",
    "                else:\n",
    "                    x = story[x:y].find(\" \") + x\n",
    "                    text = story[x:y]                \n",
    "                \n",
    "                text = text_clean_ending(text)\n",
    "                text = remove_new_lines(text)                    \n",
    "                \n",
    "                ## TODO: Write the predictor lines\n",
    "                \n",
    "                prompt, context2 = create_prompt_clipped(tokenizer, k, text, character, grammatical_number, 1024)\n",
    "                \n",
    "                inputs = tokenizer([prompt], max_length=1024, return_tensors=\"pt\")\n",
    "                input_ids = inputs[\"input_ids\"].to(device)\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    summary_ids = model.generate(input_ids, min_length=0, max_length=20)\n",
    "                    \n",
    "                out = tokenizer.decode(summary_ids[0], skip_special_tokens=True)                \n",
    "                \n",
    "                ## Ends here\n",
    "                \n",
    "                match1, match2 = exactly_or_fuzzily_matched(out, character, gold_locations)\n",
    "                result_object = ResultObject(prompt, out, ind, character, gold_locations, story_no, k, match1, match2)\n",
    "                all_result_objects[k].append(result_object)\n",
    "                \n",
    "    return all_result_objects\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da26510-473e-4a18-8ca3-71313c735f6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story11.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1179 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story12.txt\n",
      "Andersen_story13.txt\n",
      "Andersen_story15.txt\n",
      "Andersen_story16.txt\n",
      "Andersen_story17.txt\n",
      "Andersen_story18.txt\n",
      "Andersen_story1.txt\n",
      "Andersen_story2.txt\n",
      "Andersen_story3.txt\n",
      "Andersen_story5.txt\n",
      "Andersen_story7.txt\n",
      "Andersen_story8.txt\n",
      "Andersen_story9.txt\n",
      "Andersen_story10.txt\n"
     ]
    }
   ],
   "source": [
    "all_result_objects = predictor(model_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49de2599-d756-4d7a-b50f-c8c89b2030c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: define a path name for saving the results\n",
    "\n",
    "with open(\"bart_large_predictions.txt\", \"wb\") as f:\n",
    "    pickle.dump(all_result_objects, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d133df-47b5-4cd6-9f88-131141bd5b62",
   "metadata": {},
   "source": [
    "## Calculating the Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252d8012-3ffc-49cf-8333-a06a9ac1e0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: write the path name that has the results\n",
    "\n",
    "with open(\"bart_large_predictions.txt\", \"rb\") as f:\n",
    "    all_result_objects = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d02139fb-1958-4bb7-9a36-a90267a5e63b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_calculator(all_result_objects):\n",
    "    \n",
    "    exact_accuracies = defaultdict(list)\n",
    "    fuzzy_accuracies = defaultdict(list)\n",
    "    exact_averages = defaultdict(list)\n",
    "    fuzzy_averages = defaultdict(list)\n",
    "\n",
    "    for k in all_result_objects:\n",
    "        for result_object in all_result_objects[k]:\n",
    "            \"\"\"prompt = result_object.prompt\n",
    "            out = result_object.out\n",
    "            story_no = result_object.story_no\n",
    "            k = result_object.k\"\"\"\n",
    "            match1 = result_object.match1\n",
    "            match2 = result_object.match2\n",
    "            if match1 == \"Yes\":\n",
    "                exact_accuracies[k].append(1)\n",
    "            else:\n",
    "                exact_accuracies[k].append(0)\n",
    "            if match2 == \"Yes\":\n",
    "                fuzzy_accuracies[k].append(1)\n",
    "            else:\n",
    "                fuzzy_accuracies[k].append(0)\n",
    "            \n",
    "        exact_averages[k] = np.mean(np.array(exact_accuracies[k]))\n",
    "        fuzzy_averages[k] = np.mean(np.array(fuzzy_accuracies[k]))\n",
    "        \n",
    "    best_exact_k = np.array(list(exact_averages.values())).argmax() + 1\n",
    "    best_fuzzy_k = np.array(list(exact_averages.values())).argmax() + 1\n",
    "    \n",
    "    return exact_averages, fuzzy_averages, best_exact_k, best_fuzzy_k\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa2a240-ce18-44dd-8a3c-a0ddd029b302",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_averages, fuzzy_averages, best_exact_k, best_fuzzy_k = accuracy_calculator(all_result_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "419db42a-7ced-41fd-b7a9-43033f8936cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {1: 0.03614457831325301,\n",
       "             2: 0.0321285140562249,\n",
       "             3: 0.04819277108433735,\n",
       "             4: 0.05622489959839357,\n",
       "             5: 0.04819277108433735,\n",
       "             6: 0.04819277108433735,\n",
       "             7: 0.04819277108433735,\n",
       "             8: 0.04819277108433735,\n",
       "             9: 0.05220883534136546,\n",
       "             10: 0.04417670682730924,\n",
       "             11: 0.05220883534136546,\n",
       "             12: 0.03614457831325301,\n",
       "             13: 0.028112449799196786,\n",
       "             14: 0.06827309236947791,\n",
       "             15: 0.04819277108433735,\n",
       "             16: 0.05220883534136546,\n",
       "             17: 0.060240963855421686,\n",
       "             18: 0.060240963855421686,\n",
       "             19: 0.060240963855421686,\n",
       "             20: 0.04417670682730924,\n",
       "             21: 0.060240963855421686,\n",
       "             22: 0.05622489959839357,\n",
       "             23: 0.024096385542168676})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzzy_averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75209a2a-2a64-46ea-bf82-f8c3cf82b92d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
