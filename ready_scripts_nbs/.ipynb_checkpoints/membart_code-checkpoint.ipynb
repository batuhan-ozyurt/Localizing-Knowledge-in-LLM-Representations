{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86880fda-1125-48db-a25b-8dff503bc70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from memformers.models.membart import MemBartForConditionalGeneration\n",
    "\n",
    "import os\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from jinja2 import Template\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "\n",
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from math import log\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a681b5b8-895d-46fb-aaa9-8afaaec0b1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/')\n",
    "sys.path.append('/scratch/users/bozyurt20/hpc_run/text_injection')\n",
    "from text_injection.util_research import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5079d87-9c2c-47e0-88c6-38b553048a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_templates = 23\n",
    "\n",
    "path_andersen = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Andersen\"\n",
    "path_fanny = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Fanny Fern\"\n",
    "path_annotations = \"/kuacc/users/bozyurt20/hpc_run/ChildrenStories/Annotations\"\n",
    "\n",
    "dir_list_andersen = os.listdir(path_andersen)\n",
    "dir_list_fanny = os.listdir(path_fanny)\n",
    "dir_list_annotations = os.listdir(path_annotations)\n",
    "\n",
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bfce3922-8100-47a0-884d-aed75615e9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_annotations = {}\n",
    "\n",
    "for item in dir_list_annotations:\n",
    "    \n",
    "    f = open(os.path.join(path_annotations, item), 'r')\n",
    "    annotations = pd.read_csv(f, sep=\"\\t\")\n",
    "    annotations = annotations.values\n",
    "    f.close()\n",
    "    \n",
    "    all_annotations[item] = annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbd5aef8-df35-4712-8140-72cdd25452f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_info = {}\n",
    "for story in all_annotations:\n",
    "    story_info[story] = {}\n",
    "    for character_list in all_annotations[story]:\n",
    "        character_name = character_list[1]\n",
    "        story_info[story].setdefault(character_name, [])\n",
    "        story_info[story][character_name].append((character_list[0], character_list[2], character_list[3]))\n",
    "        story_info[story][character_name].sort(key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56af5c6-282a-43be-8682-47e2f904f27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MemBartForConditionalGeneration.from_pretrained(\"qywu/membart-large\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "memory_enhanced_instance = ModelInstance(model, tokenizer, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703da721-0f82-4125-b10b-96c4974200ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryEnhancedResults():\n",
    "    def __init__(self, prompt, out, ind, character, gold_locations, story_no, k, query_point, exact_match, fuzzy_match):\n",
    "        self.prompt = prompt\n",
    "        self.out = out\n",
    "        self.ind = ind\n",
    "        self.character = character\n",
    "        self.gold_locations = gold_locations\n",
    "        self.story_no = story_no\n",
    "        self.k = k\n",
    "        self.query_point = query_point\n",
    "        self.exact_match = exact_match\n",
    "        self.fuzzy_match = fuzzy_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76cac24f-5638-43a0-b8f9-705ccc582885",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_memory_states(memory_enhanced_instance, all_prompt_drafts):\n",
    "    \n",
    "    model = memory_enhanced_instance.model\n",
    "    tokenizer = memory_enhanced_instance.tokenizer\n",
    "    device = memory_enhanced_instance.device\n",
    "    \n",
    "    all_memory_states = []\n",
    "    memory_states = model.construct_memory(batch_size=1)\n",
    "    all_memory_states.append(memory_states)\n",
    "    \n",
    "    for prompt_draft in all_prompt_drafts:\n",
    "    \n",
    "        input_ids = torch.LongTensor([tokenizer.encode(prompt_draft)])\n",
    "        encoder_outputs = model.model.encoder(input_ids=input_ids, memory_states=memory_states, attention_mask=None)\n",
    "        memory_states = encoder_outputs.memory_states\n",
    "        all_memory_states.append(memory_states)\n",
    "        \n",
    "    return all_memory_states       \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e3c929b4-8512-4215-9791-7d45ea4ce00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(memory_enhanced_instance):\n",
    "\n",
    "    model = memory_enhanced_instance.model\n",
    "    tokenizer = memory_enhanced_instance.tokenizer\n",
    "    device = memory_enhanced_instance.device\n",
    "    \n",
    "    all_result_objects = defaultdict(list)\n",
    "\n",
    "    for item in dir_list_annotations:\n",
    "\n",
    "        print(item)\n",
    "\n",
    "        story_no = item[len(\"Andersen_story\"):-len(\".txt\")]\n",
    "\n",
    "        f = open(os.path.join(path_andersen, item), 'r') \n",
    "        story = f.read()\n",
    "        f.close()\n",
    "\n",
    "        characters = story_info[item].keys()\n",
    "        story = remove_new_lines(story)\n",
    "\n",
    "        big_prompt = \"Story:\\n\" + story\n",
    "        big_prompt_tokens = tokenizer.encode(big_prompt)\n",
    "        toks_per_segment = 985\n",
    "        no_segments = len(big_prompt_tokens) // toks_per_segment\n",
    "\n",
    "        all_tokens = [ big_prompt_tokens[toks_per_segment*i : toks_per_segment*(i+1)] for i in range(no_segments) ]\n",
    "\n",
    "        all_tokens.append(big_prompt_tokens[toks_per_segment*(no_segments): ])\n",
    "\n",
    "        all_prompt_drafts = [ tokenizer.decode(x, skip_special_tokens=True) for x in all_tokens ]\n",
    "        \n",
    "        all_memory_states = get_memory_states(memory_enhanced_instance, all_prompt_drafts)\n",
    "\n",
    "        x = 0\n",
    "        story_lens = []\n",
    "        for prompt_draft in all_prompt_drafts:\n",
    "            if prompt_draft[:len(\"Story:\\n\")] == \"Story:\\n\":\n",
    "                x += len(prompt_draft) - len(\"Story:\\n\")\n",
    "            else:\n",
    "                x += len(prompt_draft)\n",
    "            story_lens.append(x)\n",
    "\n",
    "        for i, prompt_draft in enumerate(all_prompt_drafts):\n",
    "            \n",
    "            print(i)\n",
    "            \n",
    "            for k in range(1, num_templates+1):\n",
    "\n",
    "                for character in characters:\n",
    "\n",
    "                    print(character)\n",
    "\n",
    "                    tuples = story_info[item][character]\n",
    "\n",
    "                    grammatical_number = tuples[0][2]\n",
    "\n",
    "                    if i != 0:\n",
    "                        pos = story_lens[i-1]\n",
    "                    else:\n",
    "                        pos = 0\n",
    "\n",
    "                    prompt, pos_last = create_prompt_clipped(tokenizer, k, prompt_draft, character, grammatical_number, 1009)\n",
    "                    pos += pos_last\n",
    "\n",
    "                    if prompt[-1] == \" \":\n",
    "                        prompt += \"<mask>\"\n",
    "                    else:\n",
    "                        prompt += \" <mask>\"\n",
    "\n",
    "                    check = False\n",
    "\n",
    "                    for num_tupl, tupl in enumerate(tuples):\n",
    "\n",
    "                        if pos < tupl[0]:\n",
    "                            check = True\n",
    "                            last_tupl = num_tupl\n",
    "                            break\n",
    "                        ind = tupl[0]\n",
    "                        gold_location = tupl[1]\n",
    "\n",
    "\n",
    "                    if check:\n",
    "                        input_ids = torch.LongTensor([tokenizer.encode(prompt)])\n",
    "                        encoder_outputs = model.model.encoder(input_ids=input_ids, memory_states=all_memory_states[i], attention_mask=None)\n",
    "                        #memory_states = encoder_outputs.memory_states\n",
    "\n",
    "                        with torch.no_grad():\n",
    "                            outputs = model.generate(\n",
    "                                encoder_outputs=encoder_outputs,\n",
    "                                decoder_start_token_id=tokenizer.bos_token_id,\n",
    "                                max_length=1024,\n",
    "                                num_beams=3,\n",
    "                                return_dict_in_generate=True,\n",
    "                            )\n",
    "\n",
    "                        out = tokenizer.decode(outputs.sequences[0], skip_special_tokens=True)\n",
    "\n",
    "                        match1, match2 = exactly_or_fuzzily_matched(out, character, gold_locations)\n",
    "                        result_object =  MemoryEnhancedResults(prompt, out, ind, character, gold_locations, story_no, k, query_point, match1, match2)\n",
    "                        all_result_objects[k].append(result_object)                 \n",
    "\n",
    "    return all_result_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3fa3c8-6409-4170-ac53-83c86c774d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Andersen_story11.txt\n",
      "here\n",
      "here\n",
      "here2\n",
      "here2\n",
      "here2\n",
      "here2\n",
      "here2\n"
     ]
    }
   ],
   "source": [
    "all_result_objects = predictor(memory_enhanced_instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00357e5-d5da-4f0f-afd9-34153688238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"membart_predictions.txt\", \"wb\") as f:\n",
    "    pickle.dump(all_result_objects, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
